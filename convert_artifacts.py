#!/usr/bin/env python3
"""
convert_artifacts.py - Convert god-context artifacts to NotebookLM-friendly formats

Converts JSON/XML to Markdown for optimal NotebookLM ingestion:
- code_metrics.json â†’ metrics.md (markdown table)
- repo_core_logic.xml â†’ repo_structure.md (wrapped in code block)
- call_graph.gv â†’ call_graph.txt (renamed)
- churn_hotspots.md, logical_coupling.md â†’ kept as-is
"""

import json
import shutil
from pathlib import Path

def convert_metrics(input_path: Path, output_path: Path) -> None:
    """Convert code_metrics.json to markdown table with risk scoring."""
    with open(input_path) as f:
        data = json.load(f)

    lines = [
        "# Code Metrics Summary",
        "",
        "Language-level metrics from `scc`. Higher complexity indicates more cognitive load.",
        "",
        "| Language | Files | Lines | Code | Comments | Blanks | Complexity | Risk Score |",
        "|----------|-------|-------|------|----------|--------|------------|------------|",
    ]

    for lang in sorted(data, key=lambda x: x.get("Complexity", 0), reverse=True):
        name = lang.get("Name", "Unknown")
        files = lang.get("Count", 0)
        total_lines = lang.get("Lines", 0)
        code = lang.get("Code", 0)
        comments = lang.get("Comment", 0)
        blanks = lang.get("Blank", 0)
        complexity = lang.get("Complexity", 0)

        # Risk score: complexity per 1000 lines of code
        risk = round(complexity / max(code, 1) * 1000, 1) if code > 0 else 0
        risk_emoji = "ðŸ”´" if risk > 100 else "ðŸŸ¡" if risk > 50 else "ðŸŸ¢"

        lines.append(
            f"| {name} | {files:,} | {total_lines:,} | {code:,} | "
            f"{comments:,} | {blanks:,} | {complexity:,} | {risk_emoji} {risk} |"
        )

    # Add summary
    total_files = sum(lang.get("Count", 0) for lang in data)
    total_lines = sum(lang.get("Lines", 0) for lang in data)
    total_code = sum(lang.get("Code", 0) for lang in data)
    total_complexity = sum(lang.get("Complexity", 0) for lang in data)

    lines.extend([
        "",
        "## Summary",
        "",
        f"- **Total Files**: {total_files:,}",
        f"- **Total Lines**: {total_lines:,}",
        f"- **Total Code Lines**: {total_code:,}",
        f"- **Total Complexity**: {total_complexity:,}",
        f"- **Average Complexity per 1K LOC**: {round(total_complexity / max(total_code, 1) * 1000, 1)}",
        "",
        "### Risk Score Legend",
        "",
        "- ðŸŸ¢ **Low** (< 50): Well-structured, maintainable code",
        "- ðŸŸ¡ **Medium** (50-100): May benefit from refactoring",
        "- ðŸ”´ **High** (> 100): Complex code, prioritize for review",
    ])

    output_path.write_text("\n".join(lines))
    print(f"âœ“ Converted {input_path.name} â†’ {output_path.name}")


def convert_xml_to_markdown(input_path: Path, output_path: Path) -> None:
    """Wrap XML content in markdown code block for proper parsing."""
    xml_content = input_path.read_text()

    # Check size and potentially truncate with warning
    size_mb = len(xml_content) / (1024 * 1024)

    lines = [
        "# Repository Structure (AST-Packed)",
        "",
        f"Source: `{input_path.name}` ({size_mb:.1f} MB)",
        "",
        "This is the AST-compressed representation of the codebase generated by repomix.",
        "It contains file structure, function signatures, and code content.",
        "",
        "```xml",
        xml_content,
        "```",
    ]

    output_path.write_text("\n".join(lines))
    print(f"âœ“ Converted {input_path.name} â†’ {output_path.name} ({size_mb:.1f} MB)")


def convert_graphviz(input_path: Path, output_path: Path) -> None:
    """Convert .gv to .txt with header."""
    content = input_path.read_text()

    lines = [
        "# Call Graph (Graphviz DOT format)",
        "",
        "This graph shows function call relationships.",
        "Visualize with: dot -Tpng call_graph.txt -o call_graph.png",
        "",
        content,
    ]

    output_path.write_text("\n".join(lines))
    print(f"âœ“ Converted {input_path.name} â†’ {output_path.name}")


def main():
    # Determine context directory
    candidates = [
        Path("notebooklm_context_diet"),
        Path("notebooklm_context"),
    ]

    context_dir = None
    for candidate in candidates:
        if candidate.exists():
            context_dir = candidate
            break

    if not context_dir:
        print("âŒ No context directory found. Run generate_god_context.sh first.")
        return 1

    print(f"ðŸ“ Processing artifacts in: {context_dir}/")
    print()

    # Convert metrics
    metrics_json = context_dir / "code_metrics.json"
    if metrics_json.exists():
        convert_metrics(metrics_json, context_dir / "metrics.md")
    else:
        print(f"âš  {metrics_json.name} not found, skipping")

    # Convert XML structure
    for xml_name in ["repo_core_logic.xml", "repo_structure.xml"]:
        xml_path = context_dir / xml_name
        if xml_path.exists():
            convert_xml_to_markdown(xml_path, context_dir / "repo_structure.md")
            break
    else:
        print("âš  No XML structure file found, skipping")

    # Convert call graph
    gv_path = context_dir / "call_graph.gv"
    if gv_path.exists():
        convert_graphviz(gv_path, context_dir / "call_graph.txt")
    else:
        print(f"âš  {gv_path.name} not found, skipping")

    # Verify existing markdown files
    for md_file in ["churn_hotspots.md", "logical_coupling.md"]:
        md_path = context_dir / md_file
        if md_path.exists():
            print(f"âœ“ {md_file} already in markdown format")
        else:
            print(f"âš  {md_file} not found")

    print()
    print("=" * 60)
    print("ðŸ“‹ NotebookLM Upload Checklist:")
    print("=" * 60)
    print()

    upload_files = [
        ("repo_structure.md", "Primary source - upload first"),
        ("churn_hotspots.md", "Bug risk analysis"),
        ("logical_coupling.md", "Hidden dependencies"),
        ("metrics.md", "Code complexity metrics"),
        ("call_graph.txt", "Function call relationships"),
    ]

    for filename, description in upload_files:
        filepath = context_dir / filename
        if filepath.exists():
            size = filepath.stat().st_size
            if size > 1024 * 1024:
                size_str = f"{size / (1024*1024):.1f} MB"
            else:
                size_str = f"{size / 1024:.1f} KB"
            print(f"  âœ“ {filename:<25} ({size_str:>8}) - {description}")
        else:
            print(f"  âœ— {filename:<25} - MISSING")

    print()
    print(f"ðŸ“‚ All files ready in: {context_dir.absolute()}/")
    return 0


if __name__ == "__main__":
    exit(main())
