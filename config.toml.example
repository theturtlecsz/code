# Codex Configuration Example
# Copy this file to config.toml and customize as needed

# Agent Configuration
# Configure which external AI models are available and their settings
[[agents]]
name = "claude"
command = "claude"
enabled = true
read-only = false
description = "Claude AI assistant with full capabilities"
args = ["--dangerously-skip-permissions", "-m", "claude-4.5-sonnet"]

[[agents]]
name = "claude-safe"
command = "claude"
enabled = true
read-only = true
description = "Claude AI assistant in read-only mode"
args = []

[[agents]]
name = "claude-opus-4-1"
command = "claude"
enabled = true
read-only = false
description = "Claude Opus 4.1 fallback for premium synthesis"
args = ["-m", "claude-opus-4-1-20250805"]

[[agents]]
name = "gemini"
command = "gemini"
enabled = true
read-only = false
description = "Google Gemini AI assistant"
# Primary experimental model + fallback for CLI coverage
env = { GEMINI_PRIMARY_MODEL = "gemini-2.0-flash-thinking-exp-01-21", GEMINI_FALLBACK_MODEL = "gemini-2.5-flash" }
args = ["-y"]

[[agents]]
name = "gemini-safe"
command = "gemini"
enabled = true
read-only = true
description = "Google Gemini AI assistant in read-only mode"
args = []

[[agents]]
name = "gemini-2-5-pro"
command = "gemini"
enabled = true
read-only = false
description = "Google Gemini 2.5 Pro with adjustable thinking budgets"
args = ["-m", "gemini-2.5-pro", "--thinking-budget", "0.3", "-y"]

[[agents]]
name = "gemini-2-5-flash"
command = "gemini"
enabled = true
read-only = false
description = "Google Gemini 2.5 Flash for cost-conscious retrieval"
args = ["-m", "gemini-2.5-flash", "-y"]

[[agents]]
name = "qwen"
command = "qwen"
enabled = true
read-only = false
description = "Qwen Coder assistant (tracks latest default model)"
# No default -m: let the CLI choose its current default model
# To pin a model explicitly, export QWEN_MODEL or add here, e.g.:
# args = ["-m", "qwen3-coder", "-y"]
args = ["-y"]
# Optional environment variables (either name works; mirrored automatically):
# env = { QWEN_API_KEY = "your-key", DASHSCOPE_API_KEY = "your-key" }

[[agents]]
name = "qwen-safe"
command = "qwen"
enabled = true
read-only = true
description = "Qwen Coder assistant in read-only mode"
# No -y in read-only; omit -m to track latest default. To pin:
# args = ["-m", "qwen3-coder"]
args = []

[[agents]]
name = "qwen2.5-coder-32b"
command = "qwen"
enabled = false
read-only = false
description = "Qwen2.5 Coder 32B offline fallback"
args = ["-m", "qwen2.5-coder-32b", "-y"]

[[agents]]
name = "rstar-l3"
command = "rstar"
enabled = false
read-only = false
description = "rStar-L3 reasoning model for air-gapped environments"
args = ["-m", "rstar-l3"]

[[agents]]
name = "codex"
command = "codex"
enabled = true
read-only = false
description = "Codex AI assistant (self-referential)"
args = ["-s", "workspace-write", "-a", "never"]

[[agents]]
name = "gpt-4"
command = "gpt"
enabled = false
read-only = false
description = "OpenAI GPT-4 assistant"
args = ["--model", "gpt-4"]
# Optional environment variables for the agent
# env = { OPENAI_API_KEY = "your-key-here" }

[[agents]]
name = "gpt-5"
canonical_name = "gpt_pro"  # SPEC-KIT-981: Used for architect/judge roles
command = "gpt"
enabled = true
read-only = false
description = "OpenAI GPT-5 router (auto reasoning)"
args = ["--model", "gpt-5", "--reasoning", "high"]

[[agents]]
name = "gpt-5-codex"
canonical_name = "gpt_codex"  # SPEC-KIT-981: Used for implementer role
command = "gpt"
enabled = true
read-only = false
description = "OpenAI GPT-5 Codex tier for code-gen ensembles"
args = ["--model", "gpt-5-codex"]

[[agents]]
name = "gpt-5-mini"
command = "gpt"
enabled = true
read-only = false
description = "OpenAI GPT-5 Mini for cost‑sensitive guardrails"
args = ["--model", "gpt-5-mini"]

[[agents]]
name = "o4-mini-high"
command = "gpt"
enabled = true
read-only = false
description = "OpenAI o4-mini-high for deterministic telemetry parsing"
args = ["--model", "o4-mini-high"]

# Custom agent example
[[agents]]
name = "custom-llm"
command = "/usr/local/bin/custom-llm"
enabled = false
read-only = false
description = "Custom LLM implementation"
args = ["--config", "/path/to/config.json"]
env = { MODEL_PATH = "/models/custom.bin", TEMP = "0.7" }

# MCP Servers

[mcp_servers.hal]
command = "npx"
args = ["-y", "hal-mcp"]
startup_timeout_sec = 20
tool_timeout_sec = 120

# Configure HAL environment by copying docs/hal/hal_config.toml.example into your
# product repository (e.g., ~/kavedarr/docs/hal/hal_config.toml) and pointing
# HAL_PROFILE at the project-specific request JSON. The Codex secret store must
# provide HAL_SECRET_KAVEDARR_API_KEY before enabling the MCP server.

# Sandbox Configuration
# Controls file system access and permissions
sandbox-mode = "workspace-write"

[sandbox-workspace-write]
writable-roots = ["."]
network-access = true
exclude-tmpdir-env-var = false
exclude-slash-tmp = false

# Shell Environment Configuration
[shell-environment-policy]
inherit = "all"
ignore-default-excludes = false
# Exclude sensitive environment variables
exclude = ["*SECRET*", "*PASSWORD*", "*CREDENTIAL*"]
# Set custom environment variables
set = { EDITOR = "vim", PAGER = "less" }

# History Configuration
[history]
persistence = "save-all"
max-bytes = 10485760  # 10MB

# MCP Server Configuration
# Example MCP server configurations matching Spec Kit defaults
[mcp_servers.codegraphcontext]
command = "uvx"
args = ["awslabs.git-repo-research-mcp-server@latest"]
startup_timeout_sec = 30
tool_timeout_sec = 120

[mcp_servers.doc_index]
command = "npx"
args = ["-y", "open-docs-mcp", "--docsDir", "/path/to/docs"]
startup_timeout_sec = 20
tool_timeout_sec = 60

[mcp_servers.git_status]
command = "uvx"
args = ["mcp-server-git", "--repository", "/path/to/repo"]
startup_timeout_sec = 20
tool_timeout_sec = 120
env = { "GIT_PAGER" = "cat" }

[mcp_servers.uniprof]
command = "npx"
args = ["-y", "uniprof", "mcp", "run"]
startup_timeout_sec = 30
tool_timeout_sec = 300

[mcp_servers.hal]
command = "npx"
args = ["-y", "hal-mcp"]
startup_timeout_sec = 20
tool_timeout_sec = 120

# File Opener Configuration
file-opener = "vscode"  # Options: vscode, vscode-insiders, windsurf, cursor, none

# Spec-Kit stage→agent mapping (SPEC-KIT-981)
# Override default routing (all stages default to GPT)
# Valid values: gpt_pro, gpt_codex, gemini, claude, code
[speckit.stage_agents]
# Uncomment to override defaults:
# specify = "gpt_pro"      # PRD generation
# plan = "gpt_pro"         # Architecture planning
# tasks = "gpt_pro"        # Task breakdown
# implement = "gpt_codex"  # Code generation (default: gpt_codex)
# validate = "gpt_pro"     # Validation
# audit = "gpt_pro"        # Security/quality audit
# unlock = "gpt_pro"       # Release gates
# clarify = "gpt_pro"      # Ambiguity resolution
# analyze = "gpt_pro"      # Consistency checking
# checklist = "gpt_pro"    # Quality scoring

# Subagent Commands
# Configure optional custom multi-agent slash commands.
# Planner does not ship built-in legacy commands like /plan, /solve, or /code.
#
# Fields:
# - name: command name (e.g., "review")
# - read-only: true for read-only
# - agents: array of agent names; if empty, falls back to enabled [[agents]]
# - orchestrator-instructions: extra instructions appended to Code (the coordinator)
# - agent-instructions: extra instructions appended to each agent’s prompt

[subagents]

# Custom example
[[subagents.commands]]
name = "review"
read-only = true
agents = ["claude", "gemini", "qwen"]
orchestrator-instructions = "Perform a multi-agent code review; reconcile disagreements into actionable feedback."
agent-instructions = "Provide inline comments, cite files/lines, and suggest precise diffs."
