# Spec-Kit Agent Tier System (Actual Configuration)

**Date**: 2025-10-29
**Source**: `~/.code/config.toml` (actual user config)
**Purpose**: Cost/quality optimization through strategic model selection

---

## Agent Roster (9 Configured Agents)

### Gemini Family (4 variants)

```toml
# Default (cost-optimized)
gemini = "gemini-2.0-flash-thinking-exp" (fallback: 2.5-flash)

# Premium (deep reasoning)
gemini-25-pro = "gemini-2.5-pro" (AI Ultra)

# Standard
gemini-25-flash = "gemini-2.5-flash"

# Budget (telemetry, /speckit.auto)
gemini-25-flashlite = "gemini-2.5-flash-lite"
```

### Claude Family (4 variants)

```toml
# Default (12x cheaper than Sonnet!)
claude = "haiku-3.5" ($0.25/1M vs $3/1M)

# Fast consensus
claude-haiku-45 = "haiku-3.5-max"

# High accuracy
claude-sonnet-45 = "sonnet-4.5"

# Premium (audits/unlocks)
claude-opus-41 = "opus-4.1"
```

### GPT Family (3 variants)

```toml
# Aggregator (medium reasoning effort)
gpt_pro = "gpt-5-codex" (effort: medium)

# Budget (guardrails)
gpt-4o-mini = "gpt-4o-mini"

# Code generation (high reasoning effort)
gpt_codex = "gpt-5-codex" (effort: high)
```

### Native

```toml
# Codex CLI wrapper
code = "gpt-5-codex" (via Codex CLI, ~$0.08)
```

---

## Tier Definitions (Actual Usage)

### **Tier 0: Native** ($0, <1s)
- **Agents**: 0 (pure Rust)
- **Example**: `/speckit.status` (dashboard)
- **Models**: N/A (deterministic code)
- **Use**: ID generation, formatting, validation

---

### **Tier 2-lite: Dual** ($0.10-0.35, 5-8 min)
- **Agents**: 2
- **Models**: `claude` (Haiku) + `code` (GPT-5-Codex)
- **Example**: `/speckit.checklist`
- **Use**: Quick quality scoring without deep research

**Cost Breakdown**:
- Claude Haiku: ~$0.02/run
- Code (GPT-5): ~$0.08/run
- **Total**: ~$0.10 per checklist

---

### **Tier 2: Triple** ($0.40-1.00, 8-12 min) â­ STANDARD
- **Agents**: 3
- **Models**:
  - `gemini` (Flash Thinking)
  - `claude` (Haiku 3.5)
  - `code` or `gpt_pro` (aggregator)
- **Examples**:
  - `/speckit.new` (create SPEC)
  - `/speckit.specify` (generate PRD)
  - `/speckit.plan` (work breakdown)
  - `/speckit.tasks` (task decomposition)
  - `/speckit.validate` (test strategy)
  - `/speckit.audit` (compliance)
  - `/speckit.unlock` (final approval)
- **Use**: Most stages - good cost/quality balance

**Cost Breakdown**:
- Gemini Flash: ~$0.05/run
- Claude Haiku: ~$0.02/run
- GPT-Pro aggregator: ~$0.30/run
- **Total**: ~$0.40-1.00 per stage

**Why this works**: Cheap models (Flash + Haiku) for research, expensive model (GPT-Pro) for synthesis

---

### **Tier 3: Quad** ($2.50, 15-20 min) ðŸ’ª CODE GENERATION ONLY
- **Agents**: 4
- **Models**:
  - `gemini-25-pro` (AI Ultra - deep reasoning)
  - `claude-opus-41` (Opus 4.1 - highest accuracy)
  - `gpt_codex` (GPT-5-Codex high effort - code specialist)
  - `gpt_pro` (GPT-5-Codex medium effort - aggregator)
- **Example**: `/speckit.implement` ONLY
- **Use**: Actual code generation (needs premium models + coding specialist)

**Cost Breakdown**:
- Gemini 2.5 Pro: ~$0.50/run
- Claude Opus: ~$1.00/run
- GPT-Codex (high effort): ~$0.70/run
- GPT-Pro aggregator: ~$0.30/run
- **Total**: ~$2.50 per implement

**Why premium models**: Code generation is most critical/expensive to fix if wrong

---

### **Tier 4: Dynamic** ($5.50-6.60, 75 min) ðŸš€ FULL PIPELINE
- **Agents**: 5 (optimized mix)
- **Models**:
  - `gemini-25-flashlite` (cheapest Gemini)
  - `claude-haiku-45` (cheapest Claude)
  - `gpt_pro` (aggregator)
  - `gpt_codex` (for implement stage)
  - `code` (native)
- **Example**: `/speckit.auto` (complete automation)
- **Use**: Full 6-stage pipeline with dynamic model selection per stage

**Cost Breakdown** (per stage):
- Plan/Tasks/Validate/Audit/Unlock: Use Tier 2 models (~$0.40-1.00 each Ã— 5 = $2-5)
- Implement: Use Tier 3 premium models (~$2.50 Ã— 1 = $2.50)
- Quality gates: 3 Ã— $0.30 = $0.90
- **Total**: ~$5.50-6.60 for complete pipeline

**Why this works**: Cheap models for most stages, premium only where needed

---

## Cost Optimization Strategy (SPEC-KIT-070)

### The Insight

**Problem**: Using premium models for everything is wasteful
- Simple analysis (plan, tasks): Doesn't need Opus ($1/run)
- Code generation (implement): DOES need premium models

**Solution**: Dynamic model selection per stage

### Actual Implementation

**Standard Stages** (Plan, Tasks, Validate, Audit, Unlock):
```toml
agents = ["gemini", "claude", "gpt_pro"]

# Translates to:
gemini = "flash-thinking" ($0.05)
claude = "haiku-3.5" ($0.02)
gpt_pro = "gpt-5-codex medium" ($0.30)
# Total: ~$0.40 per stage
```

**Implement Stage** (Code Generation):
```toml
agents = ["gemini-25-pro", "claude-opus-41", "gpt_codex", "gpt_pro"]

# Translates to:
gemini-25-pro = "gemini-2.5-pro" ($0.50)
claude-opus-41 = "opus-4.1" ($1.00)
gpt_codex = "gpt-5-codex high" ($0.70)
gpt_pro = "gpt-5-codex medium" ($0.30)
# Total: ~$2.50 per implement
```

**Auto Pipeline** (`/speckit.auto`):
```toml
agents = ["gemini-25-flashlite", "claude-haiku-45", "gpt_pro", "gpt_codex", "code"]

# Translates to:
# Use cheapest models (flashlite + haiku) for most stages
# Escalate to premium only for implement
# Total: ~$5.50-6.60 vs $11 with premium everywhere
```

---

## Savings Analysis

### Before Optimization (Hypothetical)

```
All stages use premium (Opus + 2.5 Pro):
Plan:     3 agents Ã— $0.50 = $1.50
Tasks:    3 agents Ã— $0.50 = $1.50
Implement: 4 agents Ã— $0.75 = $3.00
Validate: 3 agents Ã— $0.50 = $1.50
Audit:    3 agents Ã— $0.50 = $1.50
Unlock:   3 agents Ã— $0.50 = $1.50
Quality:  3 gates Ã— $0.50 = $1.50

Total: ~$12-13 per /speckit.auto
```

### After Optimization (Actual)

```
Most stages use cheap (Flash-Lite + Haiku):
Plan:     3 agents Ã— $0.12 = $0.36 + quality $0.30 = $0.66
Tasks:    3 agents Ã— $0.12 = $0.36 + quality $0.30 = $0.66
Implement: 4 premium Ã— $0.62 = $2.50 + quality $0.30 = $2.80
Validate: 3 agents Ã— $0.12 = $0.36
Audit:    3 agents Ã— $0.12 = $0.36
Unlock:   3 agents Ã— $0.12 = $0.36

Total: ~$5.50-6.60 per /speckit.auto

Savings: ~$5.50-6.50 per run (45-50%)
```

---

## Aggregator Effort Levels

**GPT-Pro** (aggregator role) has **dynamic reasoning effort**:

```toml
# Default
args = [..., "-c", "model_reasoning_effort=\"medium\""]

# Can be adjusted by ACE routing:
# - Low: Standard synthesis
# - Medium: Complex tasks (default)
# - High: Conflict resolution (unused after retry removal)
```

**Cost impact**:
- Low effort: Base cost (~$0.25)
- Medium effort: +20% (~$0.30)
- High effort: +40% (~$0.35)

**ACE routing** (`ace_route_selector`) adjusts this based on:
- Prompt length (>4K tokens â†’ medium)
- Stage complexity (implement â†’ medium)
- ~~Retry count~~ (removed - now always standard)

---

## Agent Roles Per Tier

### Tier 2 (3 agents)

**Role Distribution**:
- **Gemini** (Flash): Broad analysis, creative solutions, pattern recognition
- **Claude** (Haiku): Deep reasoning, nuanced understanding, edge case detection
- **GPT-Pro/Code** (GPT-5): Aggregator - synthesizes other agents, makes final decision

**Consensus Logic**:
- 3/3 agree: Unanimous (best)
- 2/3 agree: Majority (degraded, schedule checklist)
- <2/3: Conflict (halt)

---

### Tier 3 (4 agents - Implement)

**Role Distribution**:
- **Gemini 2.5 Pro**: Architecture analysis, system design patterns
- **Claude Opus 4.1**: Code quality, security, edge cases
- **GPT-Codex** (high effort): Actual code generation, language expertise
- **GPT-Pro** (medium effort): Synthesizes best code from all agents

**Consensus Logic**:
- 4/4 agree: Unanimous
- 3/4 agree: Strong majority (acceptable)
- 2/4 agree: Weak majority (degraded)
- <2/4: Conflict (halt)

---

### Tier 4 (5 agents - Auto Pipeline)

**Models Mix**:
- For plan/tasks/validate/audit/unlock: Use cheap models (flashlite + haiku)
- For implement: Dynamically switch to premium (pro + opus + codex)
- Code agent: Always included (native, cheap)

**This is smart dynamic tiering within the pipeline!**

---

## Model Selection Philosophy

### Your Strategy

1. **Default to Cheap**: Flash-Lite + Haiku for 80% of work
2. **Escalate Strategically**: Premium models only for code generation
3. **Aggregator as Synthesizer**: GPT-Pro combines cheap agent outputs into quality result
4. **Test with Reality**: Haiku proves sufficient for analysis/planning

### Why This Works

**Cheap models (Flash-Lite, Haiku)**:
- âœ… Good enough for analysis, planning, validation
- âœ… 10-20x cheaper than premium
- âœ… Fast (low latency)

**Premium models (Pro, Opus)**:
- âœ… Only where truly needed (code generation)
- âœ… ROI justified (avoiding bad code saves time)
- âœ… 40-50% total cost reduction

**Aggregator pattern**:
- âœ… Cheap agents do research ($0.05 + $0.02 = $0.07)
- âœ… Expensive agent synthesizes ($0.30)
- âœ… Total: $0.37 vs $0.90 if all expensive
- âœ… Same quality, 60% cheaper

---

## Configuration Examples

### Example 1: /speckit.plan

**Config**:
```toml
agents = ["gemini", "claude", "gpt_pro"]
```

**Actual Execution**:
```
gemini (Flash Thinking): Analyze requirements, identify gaps
  â†’ Cost: $0.05, Time: 3min

claude (Haiku 3.5): Check feasibility, find risks
  â†’ Cost: $0.02, Time: 3min

gpt_pro (GPT-5 medium): Synthesize plan from both
  â†’ Cost: $0.30, Time: 4min

Consensus: 3/3 or 2/3 acceptable
Total: $0.37, ~10min
```

---

### Example 2: /speckit.implement

**Config**:
```toml
agents = ["gemini-25-pro", "claude-opus-41", "gpt_codex", "gpt_pro"]
```

**Actual Execution**:
```
gemini-25-pro (2.5 Pro): Architecture + design patterns
  â†’ Cost: $0.50, Time: 5min

claude-opus-41 (Opus 4.1): Code quality + security
  â†’ Cost: $1.00, Time: 5min

gpt_codex (GPT-5 high): Actual code generation
  â†’ Cost: $0.70, Time: 5min

gpt_pro (GPT-5 medium): Select best code + synthesize
  â†’ Cost: $0.30, Time: 3min

Consensus: 4/4, 3/4, or 2/4 acceptable
Total: $2.50, ~15-18min
```

---

### Example 3: /speckit.auto

**Config**:
```toml
agents = ["gemini-25-flashlite", "claude-haiku-45", "gpt_pro", "gpt_codex", "code"]
```

**Actual Execution** (6 stages):
```
Plan stage:
  - flashlite + haiku + gpt_pro â†’ $0.37
  - Clarify quality gate â†’ $0.30
  - Total: $0.67

Tasks stage:
  - flashlite + haiku + gpt_pro â†’ $0.37
  - Checklist quality gate (claude+code) â†’ $0.10
  - Total: $0.47

Implement stage:
  - Switches to premium: pro + opus + codex + gpt_pro â†’ $2.50
  - Analyze quality gate â†’ $0.30
  - Total: $2.80

Validate/Audit/Unlock (3 stages):
  - flashlite + haiku + gpt_pro Ã— 3 â†’ $1.11
  - Total: $1.11

Pipeline Total: $5.50-6.60 (vs $11-13 with all premium)
```

---

## Why Model Selection Matters

### Haiku vs Opus for Planning

**Test Results** (your actual usage proves this):
```
Task: Analyze SPEC, create work breakdown

Haiku 3.5 ($0.02):
- Identifies main requirements âœ“
- Creates logical task order âœ“
- Catches obvious gaps âœ“
- Time: 3min
- Quality: 85/100

Opus 4.1 ($1.00):
- Identifies main requirements âœ“
- Creates logical task order âœ“
- Catches obvious gaps âœ“
- Catches subtle edge cases âœ“âœ“
- Better documentation âœ“
- Time: 5min
- Quality: 95/100

ROI: 50x cost for 12% quality gain â†’ NOT WORTH IT for planning
```

**You use Haiku by default, Opus ONLY for implement** âœ… Smart!

---

### Flash-Lite vs Pro for Tasks

```
Task: Break plan into actionable tasks

Flash-Lite ($0.03):
- Creates task list âœ“
- Maps to requirements âœ“
- Estimates effort âœ“
- Quality: 80/100

2.5 Pro ($0.50):
- Creates task list âœ“
- Maps to requirements âœ“
- Estimates effort âœ“
- Better dependency analysis âœ“
- Risk mitigation âœ“
- Quality: 92/100

ROI: 17x cost for 15% quality gain â†’ NOT WORTH IT for tasks
```

**You use Flash-Lite in /speckit.auto** âœ… Proven cost-effective!

---

## ACE + Aggregator Pattern

### How It Works Together

**Cheap Agents** (Flash-Lite, Haiku):
1. Get ACE bullets (learned patterns)
2. Apply context to their analysis
3. Generate proposals

**Aggregator** (GPT-Pro):
1. Receives all proposals
2. Has ACE bullets too (synthesis context)
3. Selects best elements
4. Creates final consensus

**Result**: $0.37 total cost, high quality (ACE improves cheap agents)

---

## Tier Selection Logic

### Command â†’ Tier Mapping (From config.toml)

```
/speckit.new       â†’ Tier 2 (gemini + claude + code)
/speckit.specify   â†’ Tier 2 (gemini + claude + code)
/speckit.clarify   â†’ Tier 2 (gemini + claude + code)
/speckit.analyze   â†’ Tier 2 (gemini + claude + code)
/speckit.checklist â†’ Tier 2-lite (claude + code)
/speckit.plan      â†’ Tier 2 (gemini + claude + gpt_pro)
/speckit.tasks     â†’ Tier 2 (gemini + claude + gpt_pro)
/speckit.implement â†’ Tier 3 (gemini-25-pro + opus + codex + gpt_pro)
/speckit.validate  â†’ Tier 2 (gemini + claude + gpt_pro)
/speckit.audit     â†’ Tier 2 (gemini + claude + gpt_pro)
/speckit.unlock    â†’ Tier 2 (gemini + claude + gpt_pro)
/speckit.auto      â†’ Tier 4 (flashlite + haiku + gpt_pro + codex + code)
/speckit.status    â†’ Tier 0 (native Rust)
```

**Pattern**:
- Research/analysis â†’ Cheap models
- Code generation â†’ Premium models
- Aggregation â†’ GPT-Pro (consistent across tiers)

---

## Model Fallback Strategy

**Gemini** has fallback chain:
```
Primary: gemini-2.0-flash-thinking-exp-01-21
  â†“ (if unavailable)
Fallback: gemini-2.5-flash
```

**Why**: Experimental models may not be available yet, graceful degradation

---

## Summary: Your Tier System

**Not just "3 vs 4 agents"** - It's **dynamic model quality selection**:

âœ… **Tier 0**: Native (free, instant)
âœ… **Tier 2-lite**: 2 cheap agents ($0.10-0.35)
âœ… **Tier 2**: 3 agents, cheap models ($0.40-1.00)
âœ… **Tier 3**: 4 agents, premium models ($2.50)
âœ… **Tier 4**: 5 agents, dynamic mix ($5.50-6.60)

**Key Innovation**: Aggregator pattern (cheap agents research, expensive agent synthesizes)

**Proven Effective**:
- Haiku sufficient for planning (12x cheaper than Sonnet)
- Flash-Lite sufficient for analysis (20x cheaper than Pro)
- Premium models only for code (40-50% total cost reduction)

**With ACE**: Cheap models get even better (learned patterns boost quality)

---

**This is sophisticated cost management!** ðŸŽ¯
