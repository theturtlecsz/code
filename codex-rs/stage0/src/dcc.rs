//! Dynamic Context Compiler (DCC) for Stage0
//!
//! V1.4: Implements the DCC pipeline:
//! 1. Build IQO (Intent Query Object) from spec + environment
//! 2. Query local-memory via trait abstraction
//! 3. Join with overlay scores, compute combined relevance
//! 4. Apply MMR diversity reranking
//! 5. Assemble TASK_BRIEF.md
//!
//! See STAGE0_SCORING_AND_DCC.md and STAGE0_IQO_PROMPT.md for specifications.

use crate::config::Stage0Config;
use crate::errors::Result;
use crate::guardians::LlmClient;
use crate::overlay_db::OverlayDb;
use crate::scoring::{ScoringInput, calculate_dynamic_score};
use crate::vector::{VectorBackend, VectorFilters, DocumentKind};
use async_trait::async_trait;
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::cmp::Ordering;
use std::collections::HashMap;

// ─────────────────────────────────────────────────────────────────────────────
// Data Types
// ─────────────────────────────────────────────────────────────────────────────

/// Intent Query Object - shapes how we query local-memory
///
/// Generated by LLM or heuristics from spec content and environment.
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct Iqo {
    /// Knowledge domains to filter (e.g., ["spec-kit", "infrastructure"])
    pub domains: Vec<String>,
    /// Tags that MUST be present (e.g., ["spec:SPEC-KIT-102"])
    pub required_tags: Vec<String>,
    /// Tags that boost relevance if present
    pub optional_tags: Vec<String>,
    /// Keywords for semantic search
    pub keywords: Vec<String>,
    /// Maximum candidates to retrieve before scoring
    pub max_candidates: usize,
    /// Focus areas for Tier 2 routing (used in V1.5)
    #[serde(default)]
    pub notebook_focus: Vec<String>,
}

/// Environment context passed to IQO generation
#[derive(Debug, Clone, Default)]
pub struct EnvCtx {
    /// Current working directory
    pub cwd: String,
    /// Git branch name if available
    pub branch: Option<String>,
    /// Recently modified files
    pub recent_files: Vec<String>,
}

/// Summary of a memory returned from local-memory search
#[derive(Debug, Clone)]
pub struct LocalMemorySummary {
    /// Memory ID from local-memory
    pub id: String,
    /// Domain tag if known
    pub domain: Option<String>,
    /// All tags
    pub tags: Vec<String>,
    /// Creation timestamp if available
    pub created_at: Option<DateTime<Utc>>,
    /// Short content excerpt
    pub snippet: String,
    /// Similarity score from search (0.0 to 1.0)
    pub similarity_score: f64,
}

/// Search parameters for local-memory query
#[derive(Debug, Clone)]
pub struct LocalMemorySearchParams {
    /// The IQO to use for filtering/ranking
    pub iqo: Iqo,
    /// Maximum results to return
    pub max_results: usize,
}

/// Trait for local-memory client abstraction
///
/// Stage0 uses this trait; codex-rs provides the concrete implementation
/// using MCP or REST.
#[async_trait]
pub trait LocalMemoryClient: Send + Sync {
    /// Search memories based on IQO parameters
    async fn search_memories(
        &self,
        params: LocalMemorySearchParams,
    ) -> Result<Vec<LocalMemorySummary>>;
}

/// A candidate memory with combined scoring
#[derive(Debug, Clone)]
pub struct MemoryCandidate {
    /// Memory ID from local-memory
    pub id: String,
    /// Domain if known
    pub domain: Option<String>,
    /// Tags from local-memory
    pub tags: Vec<String>,
    /// Creation time if available
    pub created_at: Option<DateTime<Utc>>,
    /// Content excerpt for TASK_BRIEF
    pub snippet: String,
    /// Similarity score from local-memory search
    pub similarity_score: f64,
    /// Dynamic score from overlay
    pub dynamic_score: f64,
    /// Vector/TF-IDF score from hybrid retrieval (V2.5)
    pub vector_score: f64,
    /// Combined score (weighted sum of similarity + dynamic + vector)
    pub combined_score: f64,
}

/// Score breakdown for explainability
#[derive(Debug, Clone, Serialize)]
pub struct ExplainScore {
    pub id: String,
    pub similarity: f64,
    pub dynamic_score: f64,
    /// Vector/TF-IDF score from hybrid retrieval (V2.5)
    pub vector_score: f64,
    pub combined_score: f64,
    pub usage_score: f64,
    pub recency_score: f64,
    pub priority_score: f64,
    pub age_penalty: f64,
    pub novelty_factor: f64,
    pub base_score: f64,
}

/// Collection of explain scores for all candidates
#[derive(Debug, Clone, Serialize)]
pub struct ExplainScores {
    pub memories: Vec<ExplainScore>,
}

/// Result of DCC compilation
#[derive(Debug, Clone)]
pub struct CompileContextResult {
    /// Generated TASK_BRIEF.md content
    pub task_brief_md: String,
    /// IDs of memories selected (in selection order)
    pub memories_used: Vec<String>,
    /// Score breakdown if explain=true
    pub explain_scores: Option<ExplainScores>,
}

// ─────────────────────────────────────────────────────────────────────────────
// DCC Context
// ─────────────────────────────────────────────────────────────────────────────

/// Context for DCC operations
pub struct DccContext<'a, Lm: LocalMemoryClient, Ll: LlmClient> {
    pub cfg: &'a Stage0Config,
    pub db: &'a OverlayDb,
    pub local_mem: &'a Lm,
    pub llm: &'a Ll,
}

/// V2.5: No-op vector backend for when hybrid retrieval is disabled or unavailable
///
/// This struct implements VectorBackend but always returns empty results.
/// Used as a type placeholder when no actual vector backend is configured.
pub struct NoopVectorBackend;

#[async_trait]
impl VectorBackend for NoopVectorBackend {
    async fn index_documents(&self, _docs: Vec<crate::vector::VectorDocument>) -> Result<crate::vector::IndexStats> {
        Ok(crate::vector::IndexStats::default())
    }

    async fn search(
        &self,
        _query_text: &str,
        _filters: &VectorFilters,
        _top_k: usize,
    ) -> Result<Vec<crate::vector::ScoredVector>> {
        Ok(Vec::new())
    }

    async fn document_count(&self) -> Result<usize> {
        Ok(0)
    }

    async fn clear(&self) -> Result<()> {
        Ok(())
    }

    async fn get_document(&self, _id: &str) -> Result<Option<crate::vector::VectorDocument>> {
        Ok(None)
    }

    async fn delete_document(&self, _id: &str) -> Result<bool> {
        Ok(false)
    }
}

// ─────────────────────────────────────────────────────────────────────────────
// IQO Generation
// ─────────────────────────────────────────────────────────────────────────────

/// Build an IQO from spec content and environment
///
/// Uses LLM if enabled in config, otherwise falls back to heuristics.
pub async fn build_iqo<L: LlmClient>(
    llm: &L,
    cfg: &Stage0Config,
    spec_content: &str,
    env: &EnvCtx,
) -> Result<Iqo> {
    if !cfg.context_compiler.iqo_llm_enabled {
        return Ok(heuristic_iqo(spec_content, cfg));
    }

    match llm.generate_iqo(spec_content, env).await {
        Ok(iqo) => Ok(normalize_iqo(iqo, cfg)),
        Err(e) => {
            tracing::warn!(
                error = %e,
                "IQO generation failed, falling back to heuristics"
            );
            Ok(heuristic_iqo(spec_content, cfg))
        }
    }
}

/// Generate a heuristic IQO without LLM
fn heuristic_iqo(spec_content: &str, cfg: &Stage0Config) -> Iqo {
    let keywords = heuristic_keywords(spec_content);
    let max_candidates = cfg.context_compiler.pre_filter_limit.min(150);

    Iqo {
        domains: vec!["spec-kit".to_string()],
        required_tags: vec![],
        optional_tags: vec![],
        keywords,
        max_candidates,
        notebook_focus: vec![],
    }
}

/// Extract keywords from spec content using simple heuristics
fn heuristic_keywords(spec: &str) -> Vec<String> {
    // Common stopwords to filter out
    const STOPWORDS: &[&str] = &[
        "the", "a", "an", "is", "are", "was", "were", "be", "been", "being",
        "have", "has", "had", "do", "does", "did", "will", "would", "could",
        "should", "may", "might", "must", "shall", "can", "need", "to", "of",
        "in", "for", "on", "with", "at", "by", "from", "as", "into", "through",
        "and", "or", "but", "if", "then", "else", "when", "where", "why", "how",
        "all", "each", "every", "both", "few", "more", "most", "other", "some",
        "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too",
        "very", "just", "also", "now", "here", "there", "this", "that", "these",
        "those", "it", "its", "we", "our", "you", "your", "they", "their", "them",
        "using", "implement", "create", "add", "new", "get", "set", "use",
    ];

    let stopword_set: std::collections::HashSet<&str> = STOPWORDS.iter().copied().collect();

    // Take first few meaningful lines
    spec.lines()
        .take(10)
        .flat_map(|line| line.split_whitespace())
        .map(|word| {
            word.trim_matches(|c: char| !c.is_alphanumeric())
                .to_lowercase()
        })
        .filter(|word| {
            word.len() > 2
                && !word.chars().all(char::is_numeric)
                && !stopword_set.contains(word.as_str())
        })
        .take(12)
        .collect()
}

/// Normalize and clamp IQO values
fn normalize_iqo(mut iqo: Iqo, cfg: &Stage0Config) -> Iqo {
    let limit = cfg.context_compiler.pre_filter_limit.min(150);
    if iqo.max_candidates == 0 || iqo.max_candidates > limit {
        iqo.max_candidates = limit;
    }

    // Ensure notebook_focus only contains known values
    iqo.notebook_focus
        .retain(|f| matches!(f.as_str(), "architecture" | "bugs" | "diary"));

    iqo
}

// ─────────────────────────────────────────────────────────────────────────────
// DCC Pipeline
// ─────────────────────────────────────────────────────────────────────────────

/// Main DCC entry point: compile context from spec and environment
///
/// # Steps
/// 1. Build IQO from spec + env
/// 2. Query local-memory
/// 3. (V2.5) Query vector backend for hybrid retrieval
/// 4. Merge and join with overlay scores
/// 5. Apply MMR diversity reranking
/// 6. Assemble TASK_BRIEF.md
pub async fn compile_context<Lm, Ll, V>(
    ctx: &DccContext<'_, Lm, Ll>,
    vector: Option<&V>,
    spec_id: &str,
    spec_content: &str,
    env: &EnvCtx,
    explain: bool,
    now: DateTime<Utc>,
) -> Result<CompileContextResult>
where
    Lm: LocalMemoryClient,
    Ll: LlmClient,
    V: VectorBackend,
{
    // 1. Build IQO
    let iqo = build_iqo(ctx.llm, ctx.cfg, spec_content, env).await?;

    tracing::debug!(
        keywords = ?iqo.keywords,
        domains = ?iqo.domains,
        max_candidates = iqo.max_candidates,
        "Built IQO"
    );

    // 2. Query local-memory
    let search_params = LocalMemorySearchParams {
        iqo: iqo.clone(),
        max_results: ctx.cfg.context_compiler.pre_filter_limit,
    };
    let summaries = ctx.local_mem.search_memories(search_params).await?;

    tracing::debug!(
        count = summaries.len(),
        "Retrieved memory summaries from local-memory"
    );

    // 3. (V2.5) Query vector backend if hybrid enabled
    let vector_scores = if ctx.cfg.context_compiler.hybrid_enabled {
        if let Some(vec_backend) = vector {
            // Build query from spec keywords
            let query_text = iqo.keywords.join(" ");
            let filters = VectorFilters::new()
                .with_kinds(vec![DocumentKind::Memory]);

            match vec_backend.search(&query_text, &filters, ctx.cfg.context_compiler.vector_top_k).await {
                Ok(results) => {
                    tracing::debug!(
                        count = results.len(),
                        "Retrieved vector search results"
                    );
                    // Build map of id -> score
                    results.into_iter().map(|sv| (sv.id, sv.score)).collect::<HashMap<String, f64>>()
                }
                Err(e) => {
                    tracing::warn!(error = %e, "Vector search failed, continuing without hybrid scores");
                    HashMap::new()
                }
            }
        } else {
            tracing::debug!("Hybrid enabled but no vector backend provided");
            HashMap::new()
        }
    } else {
        HashMap::new()
    };

    // 4. Join with overlay and compute combined scores (including vector scores)
    let mut candidates: Vec<MemoryCandidate> = Vec::new();
    let mut explain_scores: Vec<ExplainScore> = Vec::new();

    // Get weight configuration
    let sim_weight = ctx.cfg.context_compiler.semantic_similarity_weight as f64;
    let dyn_weight = ctx.cfg.context_compiler.dynamic_score_weight as f64;
    let vec_weight = if ctx.cfg.context_compiler.hybrid_enabled {
        ctx.cfg.context_compiler.vector_weight as f64
    } else {
        0.0
    };

    // Normalize weights if vector is enabled (should sum to ~1.0)
    let total_weight = sim_weight + dyn_weight + vec_weight;
    let norm_sim = sim_weight / total_weight;
    let norm_dyn = dyn_weight / total_weight;
    let norm_vec = vec_weight / total_weight;

    for s in summaries {
        // Fetch overlay row or use defaults
        let overlay = ctx.db.get_memory(&s.id)?;

        let (usage_count, initial_priority, last_accessed_at) = match &overlay {
            Some(o) => (o.usage_count as u32, o.initial_priority, o.last_accessed_at),
            None => (0, 5, None), // Default priority 5 for unknown memories
        };

        // Use provided created_at or fall back to now
        let created_at = s.created_at.unwrap_or(now);

        let scoring_input = ScoringInput::new(
            usage_count,
            initial_priority,
            last_accessed_at,
            created_at,
        );
        let components = calculate_dynamic_score(&scoring_input, &ctx.cfg.scoring, now);

        // Look up vector score (default 0.0 if not found)
        let vector_score = vector_scores.get(&s.id).copied().unwrap_or(0.0);

        // Combined score with normalized weights
        let combined = norm_sim * s.similarity_score
            + norm_dyn * components.final_score
            + norm_vec * vector_score;

        candidates.push(MemoryCandidate {
            id: s.id.clone(),
            domain: s.domain.clone(),
            tags: s.tags.clone(),
            created_at: Some(created_at),
            snippet: s.snippet.clone(),
            similarity_score: s.similarity_score,
            dynamic_score: components.final_score,
            vector_score,
            combined_score: combined,
        });

        if explain {
            explain_scores.push(ExplainScore {
                id: s.id.clone(),
                similarity: s.similarity_score,
                dynamic_score: components.final_score,
                vector_score,
                combined_score: combined,
                usage_score: components.usage_score,
                recency_score: components.recency_score,
                priority_score: components.priority_score,
                age_penalty: components.age_penalty,
                novelty_factor: components.novelty_factor,
                base_score: components.base_score,
            });
        }
    }

    // 5. Sort by combined_score descending
    candidates.sort_by(|a, b| {
        b.combined_score
            .partial_cmp(&a.combined_score)
            .unwrap_or(Ordering::Equal)
    });

    // 6. Apply MMR diversity reranking
    let selected = select_with_mmr(
        candidates,
        ctx.cfg.context_compiler.top_k,
        ctx.cfg.context_compiler.diversity_lambda as f64,
    );

    tracing::debug!(
        selected_count = selected.len(),
        top_k = ctx.cfg.context_compiler.top_k,
        hybrid_enabled = ctx.cfg.context_compiler.hybrid_enabled,
        vector_candidates = vector_scores.len(),
        "Applied MMR selection"
    );

    // 7. Assemble TASK_BRIEF.md
    let task_brief_md = assemble_task_brief(spec_id, spec_content, &selected, &iqo, ctx.cfg);

    let memories_used: Vec<String> = selected.iter().map(|c| c.id.clone()).collect();

    let explain_opt = if explain {
        Some(ExplainScores {
            memories: explain_scores,
        })
    } else {
        None
    };

    Ok(CompileContextResult {
        task_brief_md,
        memories_used,
        explain_scores: explain_opt,
    })
}

// ─────────────────────────────────────────────────────────────────────────────
// MMR Diversity Reranking
// ─────────────────────────────────────────────────────────────────────────────

/// Compute pairwise similarity between candidates
///
/// For V1.4 without vector DB, we approximate using similarity scores.
/// When vector DB is integrated, this can use real cosine similarity.
fn pairwise_similarity(a: &MemoryCandidate, b: &MemoryCandidate) -> f64 {
    // Heuristic: if both have high similarity to the query, they're likely similar to each other
    // This is a rough approximation; real implementation would use embeddings
    let base_sim = a.similarity_score.min(b.similarity_score);

    // Significant boost for shared tags (indicates topical overlap/redundancy)
    // Shared tags are a strong signal that two memories cover similar ground
    let shared_tags = a.tags.iter().filter(|t| b.tags.contains(t)).count();
    let tag_boost = if shared_tags > 0 {
        // 0.25 per shared tag, up to 0.75 boost - makes diversity selection prefer varied tags
        0.25 * (shared_tags as f64).min(3.0)
    } else {
        0.0
    };

    (base_sim + tag_boost).min(1.0)
}

/// Select top-k candidates using Maximal Marginal Relevance (MMR)
///
/// MMR balances relevance (combined_score) against diversity (pairwise similarity).
/// lambda=1.0 → pure relevance, lambda=0.0 → pure diversity
fn select_with_mmr(
    mut candidates: Vec<MemoryCandidate>,
    top_k: usize,
    lambda: f64,
) -> Vec<MemoryCandidate> {
    let mut selected: Vec<MemoryCandidate> = Vec::new();

    while !candidates.is_empty() && selected.len() < top_k {
        let mut best_idx = 0;
        let mut best_mmr = f64::NEG_INFINITY;

        for (i, c) in candidates.iter().enumerate() {
            let diversity_penalty = if selected.is_empty() {
                0.0
            } else {
                selected
                    .iter()
                    .map(|s| pairwise_similarity(c, s))
                    .fold(0.0_f64, f64::max)
            };

            let mmr_score = lambda * c.combined_score - (1.0 - lambda) * diversity_penalty;

            if mmr_score > best_mmr {
                best_mmr = mmr_score;
                best_idx = i;
            }
        }

        selected.push(candidates.remove(best_idx));
    }

    selected
}

// ─────────────────────────────────────────────────────────────────────────────
// TASK_BRIEF Assembly
// ─────────────────────────────────────────────────────────────────────────────

/// Assemble TASK_BRIEF.md from selected memories
///
/// Follows STAGE0_TASK_BRIEF_TEMPLATE.md structure.
fn assemble_task_brief(
    spec_id: &str,
    spec_content: &str,
    selected: &[MemoryCandidate],
    iqo: &Iqo,
    cfg: &Stage0Config,
) -> String {
    let mut out = String::new();

    // Header
    out.push_str(&format!("# Task Brief: {spec_id}\n\n"));
    out.push_str(&format!(
        "_Generated by Stage0 v{}_\n\n",
        crate::VERSION
    ));

    // Section 1: Spec Snapshot
    out.push_str("## 1. Spec Snapshot\n\n");
    out.push_str("### 1.1 Summary\n\n");
    let summary_bullets = summarize_spec_to_bullets(spec_content);
    for bullet in &summary_bullets {
        out.push_str(&format!("- {bullet}\n"));
    }

    out.push_str("\n### 1.2 Key Objectives\n\n");
    // Derive objectives from summary (first 3 items)
    for (i, bullet) in summary_bullets.iter().take(3).enumerate() {
        out.push_str(&format!("{}. {}\n", i + 1, bullet));
    }

    // Section 2: Relevant Context (Memories)
    out.push_str("\n## 2. Relevant Context (Memories)\n\n");

    if selected.is_empty() {
        out.push_str("_No relevant memories found._\n\n");
    } else {
        out.push_str(&format!(
            "_Selected {} memories from {} candidates using MMR (λ={:.2})_\n\n",
            selected.len(),
            cfg.context_compiler.pre_filter_limit,
            cfg.context_compiler.diversity_lambda
        ));

        out.push_str("### 2.1 High-Priority Memories\n\n");

        for (idx, m) in selected.iter().enumerate() {
            out.push_str(&format!("#### Memory {} – `{}`\n\n", idx + 1, m.id));

            // Infer type from tags
            let mem_type = infer_memory_type(&m.tags);
            out.push_str(&format!("- **Type:** {mem_type}\n"));
            out.push_str(&format!(
                "- **Score:** {:.3} (sim={:.3}, dyn={:.3})\n",
                m.combined_score, m.similarity_score, m.dynamic_score
            ));

            if !m.tags.is_empty() {
                out.push_str(&format!("- **Tags:** {}\n", m.tags.join(", ")));
            }

            if let Some(domain) = &m.domain {
                out.push_str(&format!("- **Domain:** {domain}\n"));
            }

            out.push_str(&format!("\n**Summary:**\n\n{}\n\n", m.snippet));
        }
    }

    // Section 3: Code Context (placeholder for V1.4)
    out.push_str("## 3. Code Context\n\n");
    out.push_str("_Code context extraction not implemented in V1.4._\n\n");

    // Section 4: Documentation Context (placeholder for V1.4)
    out.push_str("## 4. Documentation Context\n\n");
    out.push_str("_Documentation context extraction not implemented in V1.4._\n\n");

    // Section 5: Risks and Constraints (placeholder)
    out.push_str("## 5. Risks and Constraints\n\n");
    out.push_str("_Risk analysis not implemented in V1.4._\n\n");

    // Section 6: Suggested Approach (placeholder)
    out.push_str("## 6. Suggested Approach\n\n");
    out.push_str("_Approach suggestions will be generated by Tier 2 in V1.5._\n\n");

    // Section 7: Metadata
    out.push_str("---\n\n## 7. Metadata\n\n```json\n");

    let metadata = serde_json::json!({
        "spec_id": spec_id,
        "stage0_version": crate::VERSION,
        "dcc_config": {
            "max_tokens": cfg.context_compiler.max_tokens,
            "top_k": cfg.context_compiler.top_k,
            "pre_filter_limit": cfg.context_compiler.pre_filter_limit,
            "similarity_weight": cfg.context_compiler.semantic_similarity_weight,
            "dynamic_score_weight": cfg.context_compiler.dynamic_score_weight,
            "diversity_lambda": cfg.context_compiler.diversity_lambda,
            "iqo_llm_enabled": cfg.context_compiler.iqo_llm_enabled
        },
        "iqo": {
            "domains": iqo.domains,
            "keywords": iqo.keywords,
            "required_tags": iqo.required_tags,
            "optional_tags": iqo.optional_tags,
            "max_candidates": iqo.max_candidates
        },
        "memories_used": selected.iter().map(|m| {
            serde_json::json!({
                "id": m.id,
                "similarity": m.similarity_score,
                "dynamic_score": m.dynamic_score,
                "combined_score": m.combined_score
            })
        }).collect::<Vec<_>>()
    });

    out.push_str(&serde_json::to_string_pretty(&metadata).unwrap_or_else(|_| "{}".to_string()));
    out.push_str("\n```\n");

    out
}

/// Extract summary bullets from spec content
fn summarize_spec_to_bullets(spec: &str) -> Vec<String> {
    spec.lines()
        .filter(|l| !l.trim().is_empty())
        .take(5)
        .map(|l| {
            let trimmed = l.trim();
            // Remove markdown headers
            let cleaned = trimmed.trim_start_matches('#').trim();
            cleaned.to_string()
        })
        .filter(|s| !s.is_empty())
        .collect()
}

/// Infer memory type from tags
fn infer_memory_type(tags: &[String]) -> &'static str {
    for tag in tags {
        let lower = tag.to_lowercase();
        if lower.contains("pattern") {
            return "Pattern";
        }
        if lower.contains("decision") {
            return "Decision";
        }
        if lower.contains("problem") || lower.contains("issue") || lower.contains("bug") {
            return "Problem";
        }
        if lower.contains("insight") || lower.contains("learning") {
            return "Insight";
        }
    }
    "Other"
}

// ─────────────────────────────────────────────────────────────────────────────
// Tests
// ─────────────────────────────────────────────────────────────────────────────

#[cfg(test)]
mod tests {
    use super::*;
    use crate::errors::Stage0Error;
    use crate::guardians::MemoryKind;

    // Mock LocalMemoryClient
    struct MockLocalMemoryClient {
        memories: Vec<LocalMemorySummary>,
    }

    impl MockLocalMemoryClient {
        fn new(memories: Vec<LocalMemorySummary>) -> Self {
            Self { memories }
        }
    }

    #[async_trait]
    impl LocalMemoryClient for MockLocalMemoryClient {
        async fn search_memories(
            &self,
            params: LocalMemorySearchParams,
        ) -> Result<Vec<LocalMemorySummary>> {
            // Return up to max_results memories
            Ok(self
                .memories
                .iter()
                .take(params.max_results)
                .cloned()
                .collect())
        }
    }

    // Mock LlmClient that returns deterministic IQO
    struct MockLlmClient {
        iqo_response: Option<Iqo>,
    }

    impl MockLlmClient {
        fn new() -> Self {
            Self { iqo_response: None }
        }

        fn with_iqo(mut self, iqo: Iqo) -> Self {
            self.iqo_response = Some(iqo);
            self
        }
    }

    #[async_trait::async_trait]
    impl LlmClient for MockLlmClient {
        async fn classify_kind(&self, _input: &str) -> Result<MemoryKind> {
            Ok(MemoryKind::Other)
        }

        async fn restructure_template(
            &self,
            input: &str,
            _kind: MemoryKind,
        ) -> Result<String> {
            Ok(input.to_string())
        }

        async fn generate_iqo(
            &self,
            _spec_content: &str,
            _env: &EnvCtx,
        ) -> Result<Iqo> {
            match &self.iqo_response {
                Some(iqo) => Ok(iqo.clone()),
                None => Err(Stage0Error::prompt("Mock IQO not configured")),
            }
        }
    }

    fn sample_memories() -> Vec<LocalMemorySummary> {
        vec![
            LocalMemorySummary {
                id: "mem-001".to_string(),
                domain: Some("spec-kit".to_string()),
                tags: vec!["type:pattern".to_string(), "stage:implement".to_string()],
                created_at: Some(Utc::now()),
                snippet: "Pattern for handling async operations in Stage0".to_string(),
                similarity_score: 0.95,
            },
            LocalMemorySummary {
                id: "mem-002".to_string(),
                domain: Some("spec-kit".to_string()),
                tags: vec!["type:decision".to_string()],
                created_at: Some(Utc::now()),
                snippet: "Decision to use SQLite for overlay DB".to_string(),
                similarity_score: 0.85,
            },
            LocalMemorySummary {
                id: "mem-003".to_string(),
                domain: Some("infrastructure".to_string()),
                tags: vec!["type:pattern".to_string()],
                created_at: Some(Utc::now()),
                snippet: "Pattern for LLM client abstraction".to_string(),
                similarity_score: 0.80,
            },
            LocalMemorySummary {
                id: "mem-004".to_string(),
                domain: Some("spec-kit".to_string()),
                tags: vec!["type:insight".to_string()],
                created_at: Some(Utc::now()),
                snippet: "Insight about scoring formula behavior".to_string(),
                similarity_score: 0.75,
            },
            LocalMemorySummary {
                id: "mem-005".to_string(),
                domain: None,
                tags: vec![],
                created_at: None,
                snippet: "General note about project structure".to_string(),
                similarity_score: 0.60,
            },
        ]
    }

    #[test]
    fn test_heuristic_keywords() {
        let spec = "# SPEC-KIT-102: NotebookLM Integration\n\nImplement Tier 2 synthesis using NotebookLM MCP.";
        let keywords = heuristic_keywords(spec);

        assert!(!keywords.is_empty());
        assert!(keywords.len() <= 12);
        // Should extract meaningful words, not stopwords
        assert!(!keywords.contains(&"the".to_string()));
        assert!(!keywords.contains(&"using".to_string()));
    }

    #[test]
    fn test_heuristic_iqo() {
        let cfg = Stage0Config::default();
        let spec = "Implement Stage0 DCC pipeline";
        let iqo = heuristic_iqo(spec, &cfg);

        assert_eq!(iqo.domains, vec!["spec-kit"]);
        assert!(!iqo.keywords.is_empty());
        assert!(iqo.max_candidates <= 150);
    }

    #[test]
    fn test_normalize_iqo_clamps_max_candidates() {
        let cfg = Stage0Config::default();
        let mut iqo = Iqo {
            max_candidates: 1000, // Too high
            ..Default::default()
        };

        iqo = normalize_iqo(iqo, &cfg);
        assert!(iqo.max_candidates <= 150);
    }

    #[test]
    fn test_select_with_mmr_respects_top_k() {
        let candidates: Vec<MemoryCandidate> = (0..10)
            .map(|i| MemoryCandidate {
                id: format!("mem-{i:03}"),
                domain: None,
                tags: vec![],
                created_at: Some(Utc::now()),
                snippet: format!("Memory {i}"),
                similarity_score: 0.9 - (i as f64 * 0.05),
                dynamic_score: 0.5,
                vector_score: 0.0,
                combined_score: 0.7 - (i as f64 * 0.03),
            })
            .collect();

        let selected = select_with_mmr(candidates, 3, 0.7);
        assert_eq!(selected.len(), 3);
    }

    #[test]
    fn test_select_with_mmr_diversity() {
        // Two very similar candidates (same tags) and one different
        // Using lower lambda (0.4) to emphasize diversity over pure relevance
        let candidates = vec![
            MemoryCandidate {
                id: "similar-1".to_string(),
                domain: None,
                tags: vec!["common-tag".to_string()],
                created_at: Some(Utc::now()),
                snippet: "Similar memory 1".to_string(),
                similarity_score: 0.95,
                dynamic_score: 0.5,
                vector_score: 0.0,
                combined_score: 0.9,
            },
            MemoryCandidate {
                id: "similar-2".to_string(),
                domain: None,
                tags: vec!["common-tag".to_string()],
                created_at: Some(Utc::now()),
                snippet: "Similar memory 2".to_string(),
                similarity_score: 0.94,
                dynamic_score: 0.5,
                vector_score: 0.0,
                combined_score: 0.85, // Closer to diverse to make diversity matter more
            },
            MemoryCandidate {
                id: "diverse".to_string(),
                domain: None,
                tags: vec!["different-tag".to_string()],
                created_at: Some(Utc::now()),
                snippet: "Different memory".to_string(),
                similarity_score: 0.70, // Lower similarity = lower penalty from selected
                dynamic_score: 0.5,
                vector_score: 0.0,
                combined_score: 0.80,
            },
        ];

        // With lambda=0.4, diversity matters more (1-lambda = 0.6 weight on diversity)
        // similar-2: mmr = 0.4 * 0.85 - 0.6 * 1.0 = 0.34 - 0.6 = -0.26
        // diverse:   mmr = 0.4 * 0.80 - 0.6 * 0.70 = 0.32 - 0.42 = -0.10
        // diverse wins because lower similarity to selected means lower diversity penalty
        let selected = select_with_mmr(candidates, 2, 0.4);
        assert_eq!(selected.len(), 2);

        // First should be highest score
        assert_eq!(selected[0].id, "similar-1");
        // Second should prefer diverse candidate due to lower diversity penalty
        assert_eq!(selected[1].id, "diverse");
    }

    #[test]
    fn test_infer_memory_type() {
        assert_eq!(
            infer_memory_type(&["type:pattern".to_string()]),
            "Pattern"
        );
        assert_eq!(
            infer_memory_type(&["decision".to_string()]),
            "Decision"
        );
        assert_eq!(
            infer_memory_type(&["bug-report".to_string()]),
            "Problem"
        );
        assert_eq!(
            infer_memory_type(&["learning".to_string()]),
            "Insight"
        );
        assert_eq!(infer_memory_type(&["random".to_string()]), "Other");
    }

    #[test]
    fn test_assemble_task_brief_structure() {
        let cfg = Stage0Config::default();
        let spec_id = "SPEC-TEST-001";
        let spec_content = "# Test Spec\n\nThis is a test specification.";
        let iqo = Iqo::default();
        let selected = vec![MemoryCandidate {
            id: "mem-001".to_string(),
            domain: Some("test".to_string()),
            tags: vec!["type:pattern".to_string()],
            created_at: Some(Utc::now()),
            snippet: "Test memory snippet".to_string(),
            similarity_score: 0.9,
            dynamic_score: 0.7,
            vector_score: 0.0,
            combined_score: 0.85,
        }];

        let brief = assemble_task_brief(spec_id, spec_content, &selected, &iqo, &cfg);

        // Check required sections
        assert!(brief.contains("# Task Brief: SPEC-TEST-001"));
        assert!(brief.contains("## 1. Spec Snapshot"));
        assert!(brief.contains("## 2. Relevant Context (Memories)"));
        assert!(brief.contains("Memory 1 – `mem-001`"));
        assert!(brief.contains("## 7. Metadata"));
        assert!(brief.contains("\"spec_id\": \"SPEC-TEST-001\""));
    }

    #[tokio::test]
    async fn test_compile_context_produces_non_empty_task_brief() {
        let cfg = Stage0Config::default();
        let db = crate::overlay_db::OverlayDb::connect_in_memory().expect("db");

        let local_mem = MockLocalMemoryClient::new(sample_memories());
        let llm = MockLlmClient::new();

        let ctx = DccContext {
            cfg: &cfg,
            db: &db,
            local_mem: &local_mem,
            llm: &llm,
        };

        let env = EnvCtx::default();
        let noop_vector: Option<&NoopVectorBackend> = None;
        let result = compile_context(
            &ctx,
            noop_vector,
            "SPEC-KIT-102",
            "# Stage0 DCC\n\nImplement DCC pipeline.",
            &env,
            false,
            Utc::now(),
        )
        .await
        .expect("compile_context should succeed");

        assert!(!result.task_brief_md.is_empty());
        assert!(result.task_brief_md.contains("# Task Brief: SPEC-KIT-102"));
        assert!(!result.memories_used.is_empty());
        assert!(result.explain_scores.is_none()); // explain=false
    }

    #[tokio::test]
    async fn test_compile_context_respects_explain_flag() {
        let cfg = Stage0Config::default();
        let db = crate::overlay_db::OverlayDb::connect_in_memory().expect("db");

        let local_mem = MockLocalMemoryClient::new(sample_memories());
        let llm = MockLlmClient::new();

        let ctx = DccContext {
            cfg: &cfg,
            db: &db,
            local_mem: &local_mem,
            llm: &llm,
        };

        let env = EnvCtx::default();
        let noop_vector: Option<&NoopVectorBackend> = None;

        // With explain=true
        let result = compile_context(
            &ctx,
            noop_vector,
            "SPEC-TEST",
            "Test spec content",
            &env,
            true,
            Utc::now(),
        )
        .await
        .expect("compile_context should succeed");

        assert!(result.explain_scores.is_some());
        let scores = result.explain_scores.unwrap();
        assert!(!scores.memories.is_empty());

        // Check score fields are populated
        let first = &scores.memories[0];
        assert!(!first.id.is_empty());
        assert!(first.similarity >= 0.0);
        assert!(first.dynamic_score >= 0.0);
    }

    #[tokio::test]
    async fn test_compile_context_clamps_top_k() {
        let mut cfg = Stage0Config::default();
        cfg.context_compiler.top_k = 2; // Only select 2

        let db = crate::overlay_db::OverlayDb::connect_in_memory().expect("db");

        let local_mem = MockLocalMemoryClient::new(sample_memories()); // 5 memories
        let llm = MockLlmClient::new();

        let ctx = DccContext {
            cfg: &cfg,
            db: &db,
            local_mem: &local_mem,
            llm: &llm,
        };

        let env = EnvCtx::default();
        let noop_vector: Option<&NoopVectorBackend> = None;
        let result = compile_context(&ctx, noop_vector, "SPEC", "content", &env, false, Utc::now())
            .await
            .expect("compile_context should succeed");

        assert_eq!(result.memories_used.len(), 2);
    }

    #[tokio::test]
    async fn test_compile_context_uses_llm_iqo_when_configured() {
        let mut cfg = Stage0Config::default();
        cfg.context_compiler.iqo_llm_enabled = true;

        let db = crate::overlay_db::OverlayDb::connect_in_memory().expect("db");
        let local_mem = MockLocalMemoryClient::new(sample_memories());

        // LLM returns custom IQO
        let custom_iqo = Iqo {
            domains: vec!["custom-domain".to_string()],
            keywords: vec!["custom".to_string(), "keywords".to_string()],
            max_candidates: 50,
            ..Default::default()
        };
        let llm = MockLlmClient::new().with_iqo(custom_iqo);

        let ctx = DccContext {
            cfg: &cfg,
            db: &db,
            local_mem: &local_mem,
            llm: &llm,
        };

        let env = EnvCtx::default();
        let noop_vector: Option<&NoopVectorBackend> = None;
        let result = compile_context(&ctx, noop_vector, "SPEC", "content", &env, false, Utc::now())
            .await
            .expect("compile_context should succeed");

        // Should succeed and produce output
        assert!(result.task_brief_md.contains("custom-domain"));
    }

    // V2.5: Test hybrid retrieval with vector backend
    #[tokio::test]
    async fn test_compile_context_with_hybrid_enabled() {
        use crate::tfidf::TfIdfBackend;
        use crate::vector::VectorDocument;

        let mut cfg = Stage0Config::default();
        cfg.context_compiler.hybrid_enabled = true;
        cfg.context_compiler.vector_weight = 0.2;

        let db = crate::overlay_db::OverlayDb::connect_in_memory().expect("db");
        let local_mem = MockLocalMemoryClient::new(sample_memories());
        let llm = MockLlmClient::new();

        // Create and populate TF-IDF backend
        let tfidf = TfIdfBackend::new();
        let docs = vec![
            VectorDocument::new("mem-001", DocumentKind::Memory, "Pattern for async operations"),
            VectorDocument::new("mem-002", DocumentKind::Memory, "SQLite overlay database decision"),
            VectorDocument::new("mem-003", DocumentKind::Memory, "LLM client abstraction pattern"),
        ];
        tfidf.index_documents(docs).await.expect("index");

        let ctx = DccContext {
            cfg: &cfg,
            db: &db,
            local_mem: &local_mem,
            llm: &llm,
        };

        let env = EnvCtx::default();
        let result = compile_context(&ctx, Some(&tfidf), "SPEC", "async pattern", &env, true, Utc::now())
            .await
            .expect("compile_context should succeed");

        // Should have explain scores with vector_score populated
        assert!(result.explain_scores.is_some());
        let scores = result.explain_scores.unwrap();
        // At least one memory should have non-zero vector score if it matched
        let has_vector_score = scores.memories.iter().any(|s| s.vector_score > 0.0);
        assert!(has_vector_score, "Expected at least one memory with vector score");
    }

    #[tokio::test]
    async fn test_compile_context_hybrid_disabled_ignores_vector() {
        use crate::tfidf::TfIdfBackend;
        use crate::vector::VectorDocument;

        let mut cfg = Stage0Config::default();
        cfg.context_compiler.hybrid_enabled = false; // Disabled

        let db = crate::overlay_db::OverlayDb::connect_in_memory().expect("db");
        let local_mem = MockLocalMemoryClient::new(sample_memories());
        let llm = MockLlmClient::new();

        // Create and populate TF-IDF backend
        let tfidf = TfIdfBackend::new();
        let docs = vec![
            VectorDocument::new("mem-001", DocumentKind::Memory, "Pattern for async operations"),
        ];
        tfidf.index_documents(docs).await.expect("index");

        let ctx = DccContext {
            cfg: &cfg,
            db: &db,
            local_mem: &local_mem,
            llm: &llm,
        };

        let env = EnvCtx::default();
        let result = compile_context(&ctx, Some(&tfidf), "SPEC", "async pattern", &env, true, Utc::now())
            .await
            .expect("compile_context should succeed");

        // With hybrid disabled, all vector scores should be 0
        let scores = result.explain_scores.expect("explain should be present");
        assert!(scores.memories.iter().all(|s| s.vector_score == 0.0));
    }
}
