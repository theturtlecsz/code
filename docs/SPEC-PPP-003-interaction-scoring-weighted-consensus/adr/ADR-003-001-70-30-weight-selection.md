# ADR-003-001: 70/30 Weight Selection for Technical vs Interaction Quality

**Status**: Accepted
**Date**: 2025-11-16
**Deciders**: Research Team
**Related**: SPEC-PPP-003 (Interaction Scoring & Weighted Consensus)

---

## Context

PPP weighted consensus combines technical quality (completeness + correctness) with interaction quality (proactivity + personalization). We need to determine the optimal balance between these two dimensions.

**Question**: What weight ratio should be used for technical vs interaction quality?

Options considered:
1. **Equal weights** (50/50) - Treat both dimensions equally
2. **Technical-heavy** (80/20) - Prioritize correctness significantly
3. **Balanced** (70/30) - Favor correctness while accounting for UX
4. **User-configurable** - No default, require user to specify
5. **Dynamic adaptive** - Learn optimal weights from data

---

## Decision

We will use **70/30 weights** (70% technical + 30% interaction) as the default, with user-configurable override in Phase 2.

**Formula**:
```
final_score = 0.7 √ó technical_score + 0.3 √ó interaction_score
```

---

## Rationale

### 1. Coding Tools Prioritize Correctness

**Primary goal**: Generate correct, working code
**Secondary goal**: Good user experience

**User tolerance**:
- ‚úÖ Users tolerate questions if code is correct
- ‚ùå Users reject perfect UX if code has bugs

**Conclusion**: Technical quality must be weighted higher than interaction quality.

---

### 2. Machine Learning Ensemble Literature

**Standard practice**: Stronger model gets 60-80% weight in weighted ensembles

**Evidence** (from arXiv:1908.05287, "Optimizing Ensemble Weights"):
- Inverse error weighting typically yields 60/40 to 80/20 ratios
- Equal weights (50/50) rarely optimal
- Best performing ensembles: 65-75% weight on best model

**Application to PPP**:
- "Best model" = technical quality (most important)
- 70% falls within optimal range (65-75%)

---

### 3. Analogies to Multi-Objective Systems

| System | Primary Metric | Secondary Metric | Typical Ratio |
|--------|----------------|------------------|---------------|
| **Google Search** | Relevance | Diversity | ~70/30 (estimated) |
| **Amazon Recommendations** | Purchase probability | Diversity | ~70/30 (observed) |
| **AutoML Systems** | Accuracy | Interpretability | ~70/30 (typical) |
| **PPP (Proposed)** | Technical quality | Interaction quality | **70/30** |

**Pattern**: Systems consistently use ~70/30 when balancing primary objective with secondary UX dimension.

---

### 4. Empirical Validation (Scenario Analysis)

**Scenario 1**: Agent with excellent code but poor interaction
```
Agent A:
  Technical: 0.95 (excellent)
  Interaction: -0.45 (asked blocking question)

  Score (50/50): 0.5 √ó 0.95 + 0.5 √ó (-0.45) = 0.25 ‚Üê Too low
  Score (70/30): 0.7 √ó 0.95 + 0.3 √ó (-0.45) = 0.53 ‚Üê Reasonable
  Score (80/20): 0.8 √ó 0.95 + 0.2 √ó (-0.45) = 0.67 ‚Üê Too high?
```

**Analysis**:
- 50/50: Penalizes too harshly (excellent code rejected)
- 70/30: Balanced (agent still wins if code is significantly better)
- 80/20: Too forgiving of poor interaction

**Scenario 2**: Agent with good code and good interaction
```
Agent B:
  Technical: 0.85 (good)
  Interaction: 0.10 (excellent UX)

  Score (70/30): 0.7 √ó 0.85 + 0.3 √ó 0.10 = 0.625
```

**Comparison**: Agent B (0.625) beats Agent A (0.53) despite lower technical score - UX makes the difference.

**Conclusion**: 70/30 balances quality and UX appropriately.

---

### 5. Why NOT 80/20 (Technical-Heavy)

**Problem**: Interaction becomes almost irrelevant

**Example**:
```
Agent A: technical=0.80, interaction=-0.50 (terrible UX)
  Score (80/20): 0.8 √ó 0.80 + 0.2 √ó (-0.50) = 0.54

Agent B: technical=0.75, interaction=0.10 (great UX)
  Score (80/20): 0.8 √ó 0.75 + 0.2 √ó 0.10 = 0.62
```

**Still works, but**:
- 20% weight means interaction has minimal impact
- PPP framework's personalization dimension underutilized
- Users won't notice PPP benefits

**Decision**: 80/20 too conservative, undervalues PPP innovation.

---

### 6. Why NOT 50/50 (Equal Weights)

**Problem**: Over-penalizes technical excellence for poor interaction

**Example**:
```
Agent A: technical=0.95, interaction=-0.30
  Score (50/50): 0.5 √ó 0.95 + 0.5 √ó (-0.30) = 0.325

Agent B: technical=0.70, interaction=0.10
  Score (50/50): 0.5 √ó 0.70 + 0.5 √ó 0.10 = 0.40
```

**Problem**: Agent B wins despite 25-point lower technical score (0.95 vs 0.70).

**User expectation**: Correctness should matter more than UX.

**Decision**: 50/50 violates coding tool priorities.

---

### 7. Why NOT Dynamic Adaptive

**Approach**: Learn optimal weights from historical data

**Problems**:
1. **Cold start**: Need 100+ labeled runs before optimization works
2. **User variability**: Optimal weights differ per user
3. **Complexity**: Requires feedback mechanism ("which agent was best?")
4. **Interpretability**: Users don't understand why weights change

**Verdict**: Defer to Phase 3 (advanced feature, not default behavior).

---

## Consequences

### Positive

1. ‚úÖ **Interpretable**: 70/30 easy to explain ("correctness matters more, but UX counts")
2. ‚úÖ **Proven**: Aligns with ML ensemble literature (60-80% range)
3. ‚úÖ **Balanced**: Doesn't ignore either dimension (both influence outcome)
4. ‚úÖ **Conservative**: Favors correctness (safe for coding tool)
5. ‚úÖ **Analogous**: Similar to other multi-objective systems (Google, Amazon, etc.)

### Negative

1. ‚ö†Ô∏è **Arbitrary**: 70/30 not derived from data (domain expert choice)
   - Mitigation: Make user-configurable in Phase 2
   - Validation: A/B testing in Phase 3

2. ‚ö†Ô∏è **One-size-fits-all**: Same weights for all users
   - Mitigation: Per-user config in Phase 2
   - Alternative: Adaptive weights in Phase 3

3. ‚ö†Ô∏è **Stage-agnostic**: Same weights for all spec-kit stages
   - Mitigation: Stage-specific weights in Phase 2 (plan: 60/40, unlock: 80/20)

### Neutral

1. üìä **Sensitivity**: Changing weights by ¬±0.05 unlikely to flip agent selection
   - Good: Robust to small perturbations
   - Bad: Hard to validate optimal value empirically

---

## Alternatives Considered

### Alternative 1: Stage-Specific Defaults

**Idea**: Use different defaults per stage

```toml
[ppp.weights.plan]
technical = 0.6    # Exploration phase
interaction = 0.4

[ppp.weights.implement]
technical = 0.7    # Balanced

[ppp.weights.unlock]
technical = 0.8    # Correctness critical
interaction = 0.2
```

**Pros**:
- Adapts to task criticality (research-backed, arXiv:2502.19130)
- Plan stage tolerates questions, unlock stage doesn't

**Cons**:
- More complex (6 weight pairs to understand)
- Harder to explain to users

**Decision**: Defer to Phase 2 as optional enhancement.

---

### Alternative 2: User Survey

**Idea**: Ask users to rate preferred agent outputs, derive weights from responses

**Pros**:
- Data-driven (not arbitrary)
- User preferences incorporated

**Cons**:
- Requires user study (weeks of work)
- Need baseline to start collecting data

**Decision**: Defer to Phase 3 research project.

---

### Alternative 3: Grid Search Optimization

**Idea**: Try all weight combinations, pick best based on validation set

**Pseudocode**:
```python
best_weights = (0.7, 0.3)
min_error = inf

for w_tech in [0.5, 0.55, ..., 0.95]:
    w_interact = 1.0 - w_tech
    error = evaluate_on_validation_set(w_tech, w_interact)
    if error < min_error:
        min_error = error
        best_weights = (w_tech, w_interact)
```

**Pros**:
- Optimal for validation set

**Cons**:
- Requires labeled data (which agent is "best"?)
- May overfit to validation set
- No validation set available yet

**Decision**: Defer to Phase 3 (need data first).

---

## Validation Plan

### Phase 1: Expert Validation
- [ ] Review 70/30 with domain experts (software engineers)
- [ ] Confirm alignment with coding tool priorities
- [ ] Document rationale

### Phase 2: User Feedback
- [ ] Deploy with 70/30 defaults
- [ ] Collect user overrides (via config.toml)
- [ ] Identify common patterns (e.g., many users set 80/20?)

### Phase 3: Empirical Optimization
- [ ] Collect 100+ consensus decisions with user feedback
- [ ] Run grid search to find optimal weights
- [ ] Compare to 70/30 baseline
- [ ] Update defaults if significantly better (>5% improvement)

---

## Configuration

**Phase 1** (hardcoded):
```rust
const DEFAULT_TECHNICAL_WEIGHT: f32 = 0.7;
const DEFAULT_INTERACTION_WEIGHT: f32 = 0.3;
```

**Phase 2** (user-configurable):
```toml
[ppp.weights]
technical = 0.7
interaction = 0.3
```

**Phase 3** (adaptive):
```toml
[ppp.weights]
mode = "adaptive"  # Learn from user feedback
fallback_technical = 0.7
fallback_interaction = 0.3
```

---

## References

1. "Optimizing Ensemble Weights and Hyperparameters" (arXiv:1908.05287) - Inverse error weighting yields 60-80% on best model
2. "Voting or Consensus? Decision-Making in Multi-Agent Debate" (arXiv:2502.19130) - Task criticality affects consensus threshold
3. ML ensemble averaging (Wikipedia) - Weighted average standard technique
4. Google Search ranking - Estimated ~70% relevance, ~30% diversity (industry knowledge)

---

## Decision Matrix

| Weights | Correctness Focus | UX Impact | ML Literature | Interpretability | Decision |
|---------|-------------------|-----------|---------------|------------------|----------|
| 50/50 | ‚ùå Too low | ‚úÖ High | ‚ùå Uncommon | ‚úÖ Simple | ‚ùå Reject |
| 60/40 | ‚ö†Ô∏è Medium | ‚úÖ High | ‚úÖ In range | ‚úÖ Simple | ‚ö†Ô∏è Alternative |
| **70/30** | ‚úÖ **Good** | ‚úÖ **Medium** | ‚úÖ **Optimal range** | ‚úÖ **Simple** | ‚úÖ **ACCEPT** |
| 80/20 | ‚úÖ High | ‚ö†Ô∏è Low | ‚úÖ In range | ‚úÖ Simple | ‚ö†Ô∏è Alternative |
| 90/10 | ‚úÖ Very high | ‚ùå Minimal | ‚ö†Ô∏è Edge | ‚úÖ Simple | ‚ùå Reject |

**Winner**: 70/30 - Best balance across all criteria.

---

## Rollback Plan

If 70/30 proves suboptimal in production:

**Option 1**: Adjust to 80/20 (more conservative)
- Change constant from 0.7 to 0.8
- No code changes needed (just weight value)

**Option 2**: Make stage-specific
- Plan: 60/40
- Implement: 70/30
- Unlock: 80/20

**Option 3**: Dynamic per-user
- Track user overrides
- Use most common value as new default

**Trigger**: >30% of users override to different value.

