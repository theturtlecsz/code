# Spec-Kit Command Reference

Complete reference for all 13 /speckit.* commands.

---

## Overview

**Spec-Kit Framework** provides 13 commands organized by tier:
- **Tier 0** (Native): FREE, instant (<1s)
- **Tier 1** (Single Agent): ~$0.10, 3-5 min
- **Tier 2** (Multi-Agent): ~$0.35, 8-12 min
- **Tier 3** (Premium): ~$0.80, 10-12 min
- **Tier 4** (Full Pipeline): ~$2.70, 45-50 min

**Location**: `codex-rs/tui/src/chatwidget/spec_kit/commands/`

---

## Command Quick Reference

| Command | Tier | Cost | Time | Purpose |
|---------|------|------|------|---------|
| `/speckit.new` | 0 (Native) | $0 | <1s | Create SPEC |
| `/speckit.specify` | 1 (Single) | ~$0.10 | 3-5min | Draft PRD |
| `/speckit.clarify` | 0 (Native) | $0 | <1s | Detect ambiguity |
| `/speckit.analyze` | 0 (Native) | $0 | <1s | Check consistency |
| `/speckit.checklist` | 0 (Native) | $0 | <1s | Quality scoring |
| `/speckit.plan` | 2 (Multi) | ~$0.35 | 10-12min | Work breakdown |
| `/speckit.tasks` | 1 (Single) | ~$0.10 | 3-5min | Task decomposition |
| `/speckit.implement` | 2 (Code) | ~$0.11 | 8-12min | Code generation |
| `/speckit.validate` | 2 (Multi) | ~$0.35 | 10-12min | Test strategy |
| `/speckit.audit` | 3 (Premium) | ~$0.80 | 10-12min | Compliance check |
| `/speckit.unlock` | 3 (Premium) | ~$0.80 | 10-12min | Ship decision |
| `/speckit.auto` | 4 (Pipeline) | ~$2.70 | 45-50min | Full automation |
| `/speckit.status` | 0 (Native) | $0 | <1s | Status dashboard |

---

## Tier 0: Native Commands (FREE)

### /speckit.new

**Purpose**: Create new SPEC with template

**Tier**: 0 (Native, zero agents)
**Cost**: $0
**Time**: <1 second
**Agent Count**: 0

**Usage**:
```
/speckit.new <description>
```

**Examples**:
```
/speckit.new Add OAuth2 authentication with JWT tokens

/speckit.new Implement rate limiting for API endpoints using token bucket algorithm

/speckit.new Create user dashboard with activity metrics and export functionality
```

**What It Does**:
1. Generates unique SPEC-ID (e.g., SPEC-KIT-125)
2. Creates directory: `docs/SPEC-KIT-125-<slug>/`
3. Generates spec.md from template with:
   - Description as title
   - Empty objectives/scope/deliverables sections
   - Created timestamp
4. Creates subdirectories:
   - `evidence/` (for artifacts)
   - `adr/` (for architectural decisions)
5. Updates `SPEC.md` task tracker
6. Returns SPEC-ID to user

**Output**:
```
âœ… Created SPEC-KIT-125: Add OAuth2 authentication with JWT tokens

Directory: docs/SPEC-KIT-125-add-oauth2-authentication-jwt/
Files created:
- spec.md (template)
- evidence/ (directory)
- adr/ (directory)

Next steps:
- Run /speckit.specify SPEC-KIT-125 to draft comprehensive PRD
- Or run /speckit.auto SPEC-KIT-125 for full automation
```

**Implementation**: `codex-rs/tui/src/chatwidget/spec_kit/new_native.rs`

**No AI**: Uses template system and native SPEC-ID generation

---

### /speckit.clarify

**Purpose**: Detect ambiguities, vague language, missing details

**Tier**: 0 (Native heuristics)
**Cost**: $0
**Time**: <1 second
**Agent Count**: 0

**Usage**:
```
/speckit.clarify <SPEC-ID>
```

**Examples**:
```
/speckit.clarify SPEC-KIT-125
```

**What It Does**:
1. Reads spec.md
2. Runs heuristic pattern matching:
   - **Vague language**: "maybe", "probably", "should", "could"
   - **Undefined terms**: References without definitions
   - **Missing sections**: Empty objectives/scope/deliverables
   - **Ambiguous requirements**: "fast", "scalable", without metrics
3. Generates report with line numbers
4. Suggests improvements

**Output**:
```
ğŸ” Ambiguity Report: SPEC-KIT-125

Vague Language (3 issues):
â”œâ”€ Line 12: "should be fast" â†’ Specify target latency (e.g., <100ms p95)
â”œâ”€ Line 28: "probably need caching" â†’ Confirm requirement or remove
â””â”€ Line 45: "could support OAuth2" â†’ Required or optional?

Missing Details (2 issues):
â”œâ”€ Section "Success Criteria" is empty
â””â”€ Section "Acceptance Criteria" is empty

Undefined Terms (1 issue):
â””â”€ "JWT refresh flow" referenced but not defined

Recommendations:
1. Add quantitative metrics for performance requirements
2. Define all technical terms in Glossary section
3. Fill in Success Criteria and Acceptance Criteria
4. Replace modal language (should/could) with definitive statements

Quality Score: 6/10 (needs improvement)
```

**Implementation**: `codex-rs/tui/src/chatwidget/spec_kit/clarify_native.rs`

**Pattern Matching**:
```rust
const VAGUE_PATTERNS: &[&str] = &[
    "maybe", "probably", "should", "could", "might",
    "fast", "slow", "big", "small", "scalable",
    "efficient", "performant", "optimized",
];
```

---

### /speckit.analyze

**Purpose**: Consistency checking (structural diff)

**Tier**: 0 (Native)
**Cost**: $0
**Time**: <1 second
**Agent Count**: 0

**Usage**:
```
/speckit.analyze <SPEC-ID>
```

**Examples**:
```
/speckit.analyze SPEC-KIT-125
```

**What It Does**:
1. Reads spec.md, plan.md, tasks.md
2. Structural validation:
   - **ID consistency**: SPEC-ID matches in all files
   - **Cross-references**: All references valid
   - **Section coverage**: Required sections present
   - **Deliverable tracking**: All deliverables in tasks
3. Generates consistency report

**Output**:
```
ğŸ“Š Consistency Analysis: SPEC-KIT-125

ID Consistency: âœ… PASS
â”œâ”€ spec.md: SPEC-KIT-125
â”œâ”€ plan.md: SPEC-KIT-125
â””â”€ tasks.md: SPEC-KIT-125

Cross-References: âš ï¸ ISSUES (2)
â”œâ”€ spec.md line 34 references "ARCH-002" (not found)
â””â”€ plan.md line 67 references deliverable "oauth-flow.md" (not in spec)

Section Coverage: âœ… PASS
â”œâ”€ Objectives: Present
â”œâ”€ Scope: Present
â”œâ”€ Deliverables: Present (4 items)
â””â”€ Success Criteria: Present

Deliverable Tracking: âš ï¸ ISSUES (1)
â””â”€ Deliverable "token-refresh.md" in spec but missing from tasks.md

Recommendations:
1. Fix broken reference to ARCH-002 or remove
2. Add "oauth-flow.md" to deliverables list
3. Add task for "token-refresh.md" implementation

Consistency Score: 7/10 (minor issues)
```

**Implementation**: `codex-rs/tui/src/chatwidget/spec_kit/analyze_native.rs`

---

### /speckit.checklist

**Purpose**: Quality rubric scoring

**Tier**: 0 (Native)
**Cost**: $0
**Time**: <1 second
**Agent Count**: 0

**Usage**:
```
/speckit.checklist <SPEC-ID>
```

**Examples**:
```
/speckit.checklist SPEC-KIT-125
```

**What It Does**:
1. Evaluates spec against quality rubric:
   - **Completeness**: All sections filled
   - **Clarity**: Specific language, defined terms
   - **Testability**: Measurable success criteria
   - **Consistency**: No contradictions
2. Calculates scores (0-10 per category)
3. Overall grade (A-F)

**Output**:
```
ğŸ“‹ Quality Checklist: SPEC-KIT-125

Completeness (7/10):
â”œâ”€ âœ… Title and description present
â”œâ”€ âœ… Objectives defined (3 objectives)
â”œâ”€ âœ… Scope (in/out) defined
â”œâ”€ âœ… Deliverables listed (4 deliverables)
â”œâ”€ âš ï¸ Success criteria partially defined (missing metrics)
â””â”€ âŒ Acceptance criteria empty

Clarity (6/10):
â”œâ”€ âœ… Technical terms defined (OAuth2, JWT)
â”œâ”€ âš ï¸ Some vague language ("fast", "scalable")
â””â”€ âŒ Missing quantitative metrics

Testability (5/10):
â”œâ”€ âš ï¸ Success criteria present but not measurable
â”œâ”€ âŒ No test strategy defined
â””â”€ âŒ Acceptance criteria empty

Consistency (8/10):
â”œâ”€ âœ… No contradictions found
â”œâ”€ âœ… Cross-references valid
â””â”€ âš ï¸ Minor: deliverable "token-refresh.md" not in tasks

Overall Score: 6.5/10 (Grade: C)

Recommendations:
1. Add quantitative metrics to success criteria
2. Define acceptance criteria with test cases
3. Replace vague language with specific terms
4. Add test strategy section

Next Steps:
- Fix issues and re-run /speckit.checklist
- Or proceed with /speckit.auto (quality gates will catch issues)
```

**Implementation**: `codex-rs/tui/src/chatwidget/spec_kit/checklist_native.rs`

---

### /speckit.status

**Purpose**: Status dashboard (TUI widget)

**Tier**: 0 (Native)
**Cost**: $0
**Time**: <1 second
**Agent Count**: 0

**Usage**:
```
/speckit.status <SPEC-ID>
```

**Examples**:
```
/speckit.status SPEC-KIT-125
```

**What It Does**:
1. Reads workflow state
2. Displays TUI dashboard with:
   - Stage completion (checkmarks)
   - Artifacts generated
   - Evidence paths
   - Quality gate status
   - Cost tracking

**Output** (TUI widget):
```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ SPEC-KIT-125: Add OAuth2 authentication with JWT tokens    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stages:                                                     â”‚
â”‚ âœ… new      (native, $0)                                    â”‚
â”‚ âœ… specify  (1 agent, $0.10, 4m 23s)                        â”‚
â”‚ âœ… clarify  (native, $0)                                    â”‚
â”‚ âœ… analyze  (native, $0)                                    â”‚
â”‚ âœ… checklist (native, $0)                                   â”‚
â”‚ âœ… plan     (3 agents, $0.35, 11m 45s)                      â”‚
â”‚ âœ… tasks    (1 agent, $0.10, 3m 56s)                        â”‚
â”‚ ğŸ”„ implement (in progress, 2 agents, est. $0.11)            â”‚
â”‚ â³ validate  (pending)                                      â”‚
â”‚ â³ audit     (pending)                                      â”‚
â”‚ â³ unlock    (pending)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Artifacts:                                                  â”‚
â”‚ â”œâ”€ spec.md (2.3 KB)                                         â”‚
â”‚ â”œâ”€ plan.md (5.7 KB)                                         â”‚
â”‚ â”œâ”€ tasks.md (3.2 KB)                                        â”‚
â”‚ â””â”€ evidence/ (12 files, 450 KB)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Quality Gates:                                              â”‚
â”‚ â”œâ”€ Clarify: âœ… PASS (3 issues fixed)                        â”‚
â”‚ â”œâ”€ Analyze: âœ… PASS (no contradictions)                     â”‚
â”‚ â””â”€ Checklist: âš ï¸ 6.5/10 (Grade C, acceptable)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cost: $0.65 / $2.70 estimated total                         â”‚
â”‚ Time: 19m 24s / ~50m estimated total                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Press 'q' to close, 'r' to refresh
```

**Implementation**: `codex-rs/tui/src/chatwidget/spec_kit/command_handlers.rs` (status_command)

---

## Tier 1: Single-Agent Commands

### /speckit.specify

**Purpose**: Draft/refine PRD with strategic analysis

**Tier**: 1 (Single Agent)
**Cost**: ~$0.10
**Time**: 3-5 minutes
**Agent**: `gpt-5-low` (strategic reasoning)

**Usage**:
```
/speckit.specify <SPEC-ID> [additional context]
```

**Examples**:
```
/speckit.specify SPEC-KIT-125

/speckit.specify SPEC-KIT-125 Focus on security and OWASP top 10 compliance
```

**What It Does**:
1. Reads initial spec.md
2. Spawns `gpt-5-low` agent with PRD template
3. Agent analyzes and expands:
   - **Objectives**: Clear, measurable goals
   - **Scope**: Detailed in/out boundaries
   - **Deliverables**: Concrete artifacts
   - **Success Criteria**: Quantitative metrics
   - **Risks**: Potential blockers
4. Writes refined spec.md

**Output**:
```
ğŸ“ PRD Refinement (1 agent: gpt-5-low)

Agent: gpt-5-low (strategic analysis)
Time: 4m 12s
Cost: $0.09

Changes to spec.md:
â”œâ”€ Expanded Objectives (3 â†’ 5 objectives)
â”œâ”€ Detailed Scope section (+800 words)
â”œâ”€ Added Deliverables (4 concrete artifacts)
â”œâ”€ Success Criteria with metrics (p95 latency <100ms, etc.)
â”œâ”€ Risk Analysis (3 risks identified)
â””â”€ Acceptance Criteria (8 test scenarios)

Quality Score: 8.5/10 (improved from 6.5/10)

spec.md updated. Next: /speckit.plan SPEC-KIT-125
```

**Configuration**:
```toml
# ~/.code/config.toml

[quality_gates]
specify = ["code"]  # Single agent (default: gpt-5-low)
```

---

### /speckit.tasks

**Purpose**: Task decomposition from plan

**Tier**: 1 (Single Agent)
**Cost**: ~$0.10
**Time**: 3-5 minutes
**Agent**: `gpt-5-low`

**Usage**:
```
/speckit.tasks <SPEC-ID>
```

**Examples**:
```
/speckit.tasks SPEC-KIT-125
```

**What It Does**:
1. Reads plan.md
2. Spawns `gpt-5-low` for structured breakdown
3. Agent generates:
   - Task list with IDs
   - Dependencies
   - Effort estimates
   - Assignable units
4. Writes tasks.md
5. Updates SPEC.md task tracker

**Output**:
```
ğŸ“‹ Task Decomposition (1 agent: gpt-5-low)

Agent: gpt-5-low
Time: 3m 45s
Cost: $0.08

Generated tasks.md with 12 tasks:
â”œâ”€ T1: Setup OAuth2 provider configuration (2h)
â”œâ”€ T2: Implement JWT token generation (3h)
â”œâ”€ T3: Create token validation middleware (4h)
â”œâ”€ T4: Implement refresh token flow (5h)
â”œâ”€ T5: Add user session management (3h)
â”œâ”€ T6: Create login/logout endpoints (2h)
â”œâ”€ T7: Implement authorization guards (4h)
â”œâ”€ T8: Add rate limiting (3h)
â”œâ”€ T9: Write unit tests for token logic (4h)
â”œâ”€ T10: Write integration tests for auth flow (5h)
â”œâ”€ T11: Add security audit tests (3h)
â””â”€ T12: Document OAuth2 setup guide (2h)

Total effort: 40 hours
Critical path: T2 â†’ T3 â†’ T4 â†’ T10

SPEC.md task tracker updated.
Next: /speckit.implement SPEC-KIT-125
```

---

## Tier 2: Multi-Agent Commands

### /speckit.plan

**Purpose**: Work breakdown with multi-agent consensus

**Tier**: 2 (Multi-Agent)
**Cost**: ~$0.35
**Time**: 10-12 minutes
**Agents**: 3 (gemini-flash, claude-haiku, gpt-5-medium)

**Usage**:
```
/speckit.plan <SPEC-ID> [context]
```

**Examples**:
```
/speckit.plan SPEC-KIT-125

/speckit.plan SPEC-KIT-125 Consider microservices architecture
```

**What It Does**:
1. Reads spec.md
2. Spawns 3 agents concurrently
3. Each agent proposes plan independently
4. Consensus coordinator synthesizes:
   - Agreed approach (unanimous)
   - Points of disagreement
   - Recommended path (majority or best)
5. Writes plan.md

**Output**:
```
ğŸ“‹ Multi-Agent Planning (3 agents: gemini, claude, gpt-5)

Agents:
â”œâ”€ gemini-flash (completed in 9m 23s)
â”œâ”€ claude-haiku (completed in 10m 45s)
â””â”€ gpt-5-medium (completed in 11m 12s)

Consensus: 3/3 agents

Agreed Approach:
â”œâ”€ Use existing OAuth2 library (not build from scratch)
â”œâ”€ JWT with RS256 signing algorithm
â”œâ”€ Refresh token rotation for security
â”œâ”€ Redis for session storage
â””â”€ Rate limiting per user

Points of Disagreement:
â”œâ”€ Gemini: Suggested immediate token expiry (15min)
â”œâ”€ Claude: Recommended longer expiry (1h) with refresh
â””â”€ GPT-5: Proposed configurable expiry (default 30min)

Recommended: Configurable expiry (2 agents in favor)

Work Breakdown:
1. OAuth2 Provider Integration (Gemini's approach)
2. JWT Token Service (Claude's implementation pattern)
3. Session Management (GPT-5's Redis strategy)
4. Rate Limiting (Consensus: token bucket algorithm)
5. Security Audit (All agents agree: OWASP checklist)

plan.md created (5.7 KB)
Cost: $0.34
Time: 11m 45s

Next: /speckit.tasks SPEC-KIT-125
```

**Configuration**:
```toml
[quality_gates]
plan = ["gemini", "claude", "code"]  # 3 agents (balanced)
# or
plan = ["gemini", "gemini", "gemini"]  # Cheap ($0.10 total)
# or
plan = ["gemini-pro", "claude-opus", "gpt-5"]  # Premium ($1.20 total)
```

---

### /speckit.implement

**Purpose**: Code generation with specialist model

**Tier**: 2 (Specialist + Validator)
**Cost**: ~$0.11
**Time**: 8-12 minutes
**Agents**: 2 (gpt-5-codex HIGH, claude-haiku validator)

**Usage**:
```
/speckit.implement <SPEC-ID>
```

**Examples**:
```
/speckit.implement SPEC-KIT-125
```

**What It Does**:
1. Reads plan.md and tasks.md
2. Spawns `gpt-5-codex` (HIGH reasoning) for code generation
3. Spawns `claude-haiku` for validation
4. Code generation:
   - Implements all deliverables
   - Adds comprehensive docstrings
   - Includes type hints
   - Follows project conventions
5. Validation:
   - Checks code quality
   - Runs static analysis
   - Verifies tests compile
6. Writes code files

**Output**:
```
ğŸ”¨ Code Generation (2 agents: gpt-5-codex, claude-haiku)

Agent 1: gpt-5-codex (HIGH reasoning)
â””â”€ Generated code (12m 34s)

Files created:
â”œâ”€ src/auth/oauth2_provider.rs (234 lines)
â”œâ”€ src/auth/jwt_service.rs (189 lines)
â”œâ”€ src/auth/session_manager.rs (156 lines)
â”œâ”€ src/auth/middleware.rs (98 lines)
â”œâ”€ src/auth/rate_limiter.rs (145 lines)
â””â”€ tests/auth_integration_tests.rs (312 lines)

Agent 2: claude-haiku (validator)
â””â”€ Validation (3m 12s)

Validation Results:
â”œâ”€ âœ… cargo fmt --check (passed)
â”œâ”€ âœ… cargo clippy (0 warnings)
â”œâ”€ âœ… cargo build (compiled successfully)
â”œâ”€ âœ… cargo test --no-run (tests compile)
â””â”€ âœ… Code quality: 9/10

Cost: $0.11 (codex: $0.09, validator: $0.02)
Time: 15m 46s

Next: /speckit.validate SPEC-KIT-125
```

**Configuration**:
```toml
[quality_gates]
implement = ["gpt_codex", "claude"]  # Specialist + validator
# gpt_codex uses HIGH reasoning by default
```

---

### /speckit.validate

**Purpose**: Test strategy consensus

**Tier**: 2 (Multi-Agent)
**Cost**: ~$0.35
**Time**: 10-12 minutes
**Agents**: 3 (gemini-flash, claude-haiku, gpt-5-medium)

**Usage**:
```
/speckit.validate <SPEC-ID>
```

**Examples**:
```
/speckit.validate SPEC-KIT-125
```

**What It Does**:
1. Reads implementation code
2. Spawns 3 agents for test strategy
3. Each agent proposes:
   - Unit test coverage
   - Integration test scenarios
   - E2E test flows
   - Security test cases
4. Consensus on comprehensive test plan
5. Writes validation_plan.md

**Output**:
```
ğŸ§ª Test Strategy (3 agents: gemini, claude, gpt-5)

Agents:
â”œâ”€ gemini-flash (completed in 10m 12s)
â”œâ”€ claude-haiku (completed in 11m 34s)
â””â”€ gpt-5-medium (completed in 10m 56s)

Consensus: 3/3 agents

Test Coverage Strategy:
â”œâ”€ Unit Tests (all agents agree):
â”‚   â”œâ”€ JWT token generation/validation
â”‚   â”œâ”€ Session creation/retrieval
â”‚   â”œâ”€ Rate limiter logic
â”‚   â””â”€ Middleware authorization
â”‚
â”œâ”€ Integration Tests (consensus):
â”‚   â”œâ”€ Full OAuth2 flow (login â†’ token â†’ refresh â†’ logout)
â”‚   â”œâ”€ Concurrent session handling
â”‚   â”œâ”€ Rate limit enforcement across requests
â”‚   â””â”€ Token expiry and refresh scenarios
â”‚
â”œâ”€ Security Tests (all agents agree):
â”‚   â”œâ”€ OWASP A2: Broken Authentication (replay attacks, etc.)
â”‚   â”œâ”€ OWASP A3: Sensitive Data Exposure (token leakage)
â”‚   â”œâ”€ OWASP A5: Broken Access Control (unauthorized access)
â”‚   â””â”€ OWASP A7: XSS (token injection attacks)
â”‚
â””â”€ Performance Tests (GPT-5's addition, accepted by others):
    â”œâ”€ Token generation throughput (target: 1000/s)
    â”œâ”€ Session lookup latency (target: <10ms p95)
    â””â”€ Rate limiter overhead (target: <1ms)

Target Coverage: 85% line coverage (all agents agree)

validation_plan.md created (4.2 KB)
Cost: $0.34
Time: 11m 34s

Next: /speckit.audit SPEC-KIT-125
```

---

## Tier 3: Premium Commands

### /speckit.audit

**Purpose**: Compliance and security validation

**Tier**: 3 (Premium Multi-Agent)
**Cost**: ~$0.80
**Time**: 10-12 minutes
**Agents**: 3 (gemini-pro, claude-sonnet, gpt-5-high)

**Usage**:
```
/speckit.audit <SPEC-ID>
```

**Examples**:
```
/speckit.audit SPEC-KIT-125
```

**What It Does**:
1. Reads all code and tests
2. Spawns 3 premium agents for deep analysis
3. Each agent audits:
   - **Security**: OWASP top 10, CWE common weaknesses
   - **Compliance**: Standards (OAuth2 RFC, JWT RFC)
   - **Quality**: Code smells, anti-patterns
   - **Performance**: Bottlenecks, scalability
4. Consensus on findings and recommendations
5. Writes audit_report.md

**Output**:
```
ğŸ”’ Security & Compliance Audit (3 agents: gemini-pro, claude-sonnet, gpt-5-high)

Agents:
â”œâ”€ gemini-pro (completed in 11m 23s)
â”œâ”€ claude-sonnet (completed in 10m 45s)
â””â”€ gpt-5-high (completed in 12m 01s)

Consensus: 3/3 agents

Security Findings:
â”œâ”€ âœ… OWASP A2 (Broken Auth): PASS (all agents agree)
â”‚   â””â”€ Proper token validation, no replay attacks
â”œâ”€ âœ… OWASP A3 (Data Exposure): PASS (all agents agree)
â”‚   â””â”€ Tokens encrypted in transit (HTTPS), not logged
â”œâ”€ âš ï¸ OWASP A5 (Access Control): MINOR ISSUE (2/3 agents)
â”‚   â”œâ”€ Claude: Missing authorization check in /refresh endpoint
â”‚   â””â”€ GPT-5: Agrees, suggests adding user_id validation
â”œâ”€ âœ… OWASP A7 (XSS): PASS (all agents agree)
â”‚   â””â”€ Input sanitization present
â””â”€ âœ… Token Security: PASS (all agents agree)
    â””â”€ RS256 signing, proper key management

Compliance Findings:
â”œâ”€ âœ… OAuth2 RFC 6749: COMPLIANT (all agents agree)
â”œâ”€ âœ… JWT RFC 7519: COMPLIANT (all agents agree)
â””â”€ âš ï¸ Refresh Token Best Practices: MINOR DEVIATION (Gemini)
    â””â”€ Recommends token rotation on each refresh

Quality Findings:
â”œâ”€ âœ… Code Quality: 9/10 (consensus)
â”œâ”€ âœ… Test Coverage: 87% (exceeds 85% target)
â””â”€ âš ï¸ Performance: 1 bottleneck identified
    â””â”€ Redis session lookup could be cached (Claude's finding)

Critical Issues: 0
Major Issues: 0
Minor Issues: 3

Recommendations (Consensus):
1. Add user_id validation to /refresh endpoint (SECURITY)
2. Implement token rotation on refresh (BEST PRACTICE)
3. Add caching layer for session lookups (PERFORMANCE)

Audit Decision: âœ… PASS (with minor recommendations)

audit_report.md created (6.8 KB)
Cost: $0.78
Time: 12m 01s

Next: /speckit.unlock SPEC-KIT-125
```

---

### /speckit.unlock

**Purpose**: Final ship/no-ship decision

**Tier**: 3 (Premium Multi-Agent)
**Cost**: ~$0.80
**Time**: 10-12 minutes
**Agents**: 3 (gemini-pro, claude-sonnet, gpt-5-high)

**Usage**:
```
/speckit.unlock <SPEC-ID>
```

**Examples**:
```
/speckit.unlock SPEC-KIT-125
```

**What It Does**:
1. Reads all artifacts (spec, plan, code, tests, audit)
2. Spawns 3 premium agents for final review
3. Each agent evaluates:
   - **Completeness**: All deliverables present
   - **Quality**: Code meets standards
   - **Security**: No critical issues
   - **Readiness**: Production-ready
4. Consensus on ship/no-ship
5. Writes unlock_decision.md

**Output**:
```
ğŸš€ Unlock Decision (3 agents: gemini-pro, claude-sonnet, gpt-5-high)

Agents:
â”œâ”€ gemini-pro (completed in 10m 34s)
â”œâ”€ claude-sonnet (completed in 11m 12s)
â””â”€ gpt-5-high (completed in 10m 45s)

Consensus: 3/3 agents

Completeness Review:
â”œâ”€ âœ… All deliverables present (4/4)
â”œâ”€ âœ… Tests written and passing (87% coverage)
â”œâ”€ âœ… Documentation complete (OAuth2 setup guide)
â””â”€ âœ… Security audit passed

Quality Review:
â”œâ”€ âœ… Code quality: 9/10
â”œâ”€ âœ… Test quality: 8.5/10
â”œâ”€ âœ… No critical issues
â””â”€ âš ï¸ 3 minor recommendations (non-blocking)

Security Review:
â”œâ”€ âœ… OWASP top 10: PASS
â”œâ”€ âœ… OAuth2/JWT compliance: PASS
â””â”€ âš ï¸ 1 minor security recommendation (token rotation)

Readiness Review:
â”œâ”€ âœ… Production-ready (all agents agree)
â”œâ”€ âœ… Deployment plan documented
â”œâ”€ âœ… Rollback strategy defined
â””â”€ âœ… Monitoring configured

Ship Decision:
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ… SHIP APPROVED (3/3 agents)             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Gemini: SHIP âœ…
â””â”€ "Implementation is complete, secure, and well-tested. Minor recommendations can be addressed post-launch."

Claude: SHIP âœ…
â””â”€ "Code meets quality standards. Security audit passed with minor suggestions for improvement."

GPT-5: SHIP âœ…
â””â”€ "Production-ready. Excellent test coverage and documentation. Recommend addressing token rotation in v1.1."

Post-Launch TODO:
1. Monitor authentication latency metrics
2. Implement token rotation (v1.1)
3. Add session lookup caching (v1.1)

unlock_decision.md created (3.2 KB)
Cost: $0.79
Time: 11m 12s

ğŸ‰ SPEC-KIT-125 complete! Ready to ship.
```

---

## Tier 4: Full Pipeline

### /speckit.auto

**Purpose**: Full 6-stage automation pipeline

**Tier**: 4 (Strategic Routing)
**Cost**: ~$2.70 (75% cheaper than original $11)
**Time**: 45-50 minutes
**Stages**: specify â†’ plan â†’ tasks â†’ implement â†’ validate â†’ audit â†’ unlock

**Usage**:
```
/speckit.auto <SPEC-ID> [--from STAGE]
```

**Examples**:
```
/speckit.auto SPEC-KIT-125

/speckit.auto SPEC-KIT-125 --from plan  # Resume from plan stage
```

**What It Does**:
1. Runs all stages in sequence:
   - Native quality checks (FREE): clarify, analyze, checklist
   - specify (1 agent, $0.10)
   - plan (3 agents, $0.35)
   - tasks (1 agent, $0.10)
   - implement (2 agents, $0.11)
   - validate (3 agents, $0.35)
   - audit (3 premium, $0.80)
   - unlock (3 premium, $0.80)
2. Quality gates between stages
3. Auto-advancement on success
4. Stops on gate failure (manual review required)

**Output** (abbreviated):
```
ğŸ¤– Full Automation Pipeline: SPEC-KIT-125

Pipeline Stages: 8 stages (3 native + 5 multi-agent)
Estimated Cost: $2.70
Estimated Time: 45-50 minutes

[Stage 1/8] clarify (native)...
âœ… Completed in <1s ($0)
Quality Gate: âœ… PASS (2 issues found, auto-fixed)

[Stage 2/8] specify (1 agent)...
Agent: gpt-5-low
âœ… Completed in 4m 12s ($0.09)
Quality Gate: âœ… PASS (quality score 8.5/10)

[Stage 3/8] plan (3 agents)...
Agents: gemini, claude, gpt-5
âœ… Completed in 11m 45s ($0.34)
Consensus: 3/3 agents
Quality Gate: âœ… PASS (unanimous agreement)

[Stage 4/8] tasks (1 agent)...
Agent: gpt-5-low
âœ… Completed in 3m 56s ($0.08)
Quality Gate: âœ… PASS (12 tasks generated)

[Stage 5/8] implement (2 agents)...
Agents: gpt-5-codex, claude-haiku
âœ… Completed in 15m 46s ($0.11)
Validation: âœ… PASS (all checks passed)
Quality Gate: âœ… PASS

[Stage 6/8] validate (3 agents)...
Agents: gemini, claude, gpt-5
âœ… Completed in 11m 34s ($0.34)
Consensus: 3/3 agents
Quality Gate: âœ… PASS (85% coverage target met)

[Stage 7/8] audit (3 premium agents)...
Agents: gemini-pro, claude-sonnet, gpt-5-high
âœ… Completed in 12m 01s ($0.78)
Consensus: 3/3 agents (0 critical, 0 major, 3 minor issues)
Quality Gate: âœ… PASS

[Stage 8/8] unlock (3 premium agents)...
Agents: gemini-pro, claude-sonnet, gpt-5-high
âœ… Completed in 11m 12s ($0.79)
Decision: âœ… SHIP (3/3 agents approve)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‰ PIPELINE COMPLETE                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Total Cost: $2.73                           â•‘
â•‘  Total Time: 47m 23s                         â•‘
â•‘  Stages Passed: 8/8 âœ…                        â•‘
â•‘  Decision: SHIP APPROVED âœ…                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Artifacts:
â”œâ”€ spec.md (refined PRD)
â”œâ”€ plan.md (consensus work breakdown)
â”œâ”€ tasks.md (12 tasks)
â”œâ”€ src/auth/*.rs (6 files, 1134 lines)
â”œâ”€ tests/*.rs (312 lines, 87% coverage)
â”œâ”€ validation_plan.md
â”œâ”€ audit_report.md
â””â”€ unlock_decision.md

Evidence: docs/SPEC-KIT-125-.../evidence/ (28 files, 2.1 MB)

Next Steps:
1. Review artifacts
2. Address 3 minor audit recommendations (optional, non-blocking)
3. Deploy to production
```

**Resumption** (if interrupted):
```
/speckit.auto SPEC-KIT-125 --from validate

Resuming from stage 6/8 (validate)...
Previous stages: specify âœ…, plan âœ…, tasks âœ…, implement âœ…
Remaining: validate, audit, unlock
```

**Configuration**:
```toml
[quality_gates]
# Customize each stage's agents
plan = ["gemini", "claude", "code"]
tasks = ["code"]
implement = ["gpt_codex", "claude"]
validate = ["gemini", "claude", "code"]
audit = ["gemini-pro", "claude-sonnet", "gpt-5"]
unlock = ["gemini-pro", "claude-sonnet", "gpt-5"]
```

---

## Legacy Commands (Backward Compatibility)

These commands still work but are deprecated:

| Legacy Command | New Command | Status |
|----------------|-------------|--------|
| `/new-spec` | `/speckit.new` | Deprecated |
| `/spec-plan` | `/speckit.plan` | Deprecated |
| `/spec-tasks` | `/speckit.tasks` | Deprecated |
| `/spec-implement` | `/speckit.implement` | Deprecated |
| `/spec-validate` | `/speckit.validate` | Deprecated |
| `/spec-audit` | `/speckit.audit` | Deprecated |
| `/spec-unlock` | `/speckit.unlock` | Deprecated |
| `/spec-auto` | `/speckit.auto` | Deprecated |
| `/spec-status` | `/speckit.status` | Deprecated |

**Migration**: Replace `/spec-*` with `/speckit.*` in all workflows

---

## Cost Summary

### Per-Command Costs

| Command | Agents | Provider(s) | Input Tokens | Output Tokens | Cost |
|---------|--------|-------------|--------------|---------------|------|
| `new` | 0 | Native | 0 | 0 | $0.00 |
| `clarify` | 0 | Native | 0 | 0 | $0.00 |
| `analyze` | 0 | Native | 0 | 0 | $0.00 |
| `checklist` | 0 | Native | 0 | 0 | $0.00 |
| `specify` | 1 | OpenAI (gpt-5-low) | ~8K | ~3K | $0.09 |
| `plan` | 3 | Gemini+Claude+OpenAI | ~20K | ~8K | $0.34 |
| `tasks` | 1 | OpenAI (gpt-5-low) | ~12K | ~4K | $0.08 |
| `implement` | 2 | OpenAI (codex)+Claude | ~30K | ~10K | $0.11 |
| `validate` | 3 | Gemini+Claude+OpenAI | ~25K | ~8K | $0.34 |
| `audit` | 3 | Gemini Pro+Sonnet+GPT-5 | ~40K | ~12K | $0.78 |
| `unlock` | 3 | Gemini Pro+Sonnet+GPT-5 | ~35K | ~10K | $0.79 |
| `auto` | Strategic | Mixed (all above) | ~170K | ~55K | $2.73 |

### Cost Optimization Strategies

**Minimum Cost** (single cheap agent everywhere):
```toml
[quality_gates]
specify = ["gemini"]
plan = ["gemini"]
tasks = ["gemini"]
implement = ["gemini"]
validate = ["gemini"]
audit = ["gemini"]
unlock = ["gemini"]
# Total: ~$0.50 (vs $2.70)
```

**Balanced** (recommended, current default):
```toml
[quality_gates]
specify = ["code"]                        # $0.10
plan = ["gemini", "claude", "code"]       # $0.35
tasks = ["code"]                          # $0.10
implement = ["gpt_codex", "claude"]       # $0.11
validate = ["gemini", "claude", "code"]   # $0.35
audit = ["gemini-pro", "claude-sonnet", "gpt-5"]  # $0.80
unlock = ["gemini-pro", "claude-sonnet", "gpt-5"] # $0.80
# Total: ~$2.70
```

**Premium** (highest quality):
```toml
[quality_gates]
specify = ["gpt-5"]                       # $0.20
plan = ["gemini-pro", "claude-opus", "gpt-5"]    # $1.20
tasks = ["gpt-5"]                         # $0.20
implement = ["gpt_codex", "claude-opus"]  # $0.35
validate = ["gemini-pro", "claude-opus", "gpt-5"] # $1.20
audit = ["gemini-pro", "claude-opus", "gpt-5"]    # $0.80
unlock = ["gemini-pro", "claude-opus", "gpt-5"]   # $0.80
# Total: ~$4.75
```

---

## Next Steps

- [Pipeline Architecture](pipeline-architecture.md) - State machine and workflow
- [Consensus System](consensus-system.md) - Multi-agent synthesis
- [Quality Gates](quality-gates.md) - Checkpoint configuration
- [Native Operations](native-operations.md) - FREE operations deep dive

---

**File References**:
- Command implementations: `codex-rs/tui/src/chatwidget/spec_kit/commands/`
- Command registry: `codex-rs/tui/src/chatwidget/spec_kit/command_registry.rs`
- Native operations: `codex-rs/tui/src/chatwidget/spec_kit/*_native.rs`
- Auto pipeline: `codex-rs/tui/src/chatwidget/spec_kit/pipeline_coordinator.rs`
